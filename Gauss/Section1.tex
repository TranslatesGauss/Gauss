\documentclass[12pt]{memoir}
\usepackage{standalone}
\usepackage[dvips,text={6.5truein,9.1truein},left=0.86truein,right=0.8truein,top=1truein]{geometry}
\usepackage{amsmath, amsthm, amsfonts}
\usepackage{titlesec}

% Uncomment to use syncing
%\usepackage{pdfsync}


% Paragraphs
\usepackage{indentfirst}
\parindent=3em
\parskip=0pt

%font
\usepackage{mlmodern}
%\usepackage[T1]{fontenc}% http://ctan.org/pkg/fontenc
\usepackage{microtype}

\titleformat{\section}
 {\centering}{\thesection.}{0em}{}

\titleformat{\subsection}
 {\normalfont\small\centering}{\thesection.}{0em}{}
\titlespacing*{\subsection}
{0pt}{\baselineskip}{0\baselineskip}

%footnotes
\usepackage[perpage]{footmisc}
\usepackage{etoolbox}
\DefineFNsymbols*{asterisks}{{ *}{ **}{ ***}}
\setfnsymbol{asterisks}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\makeatletter
\renewcommand{\@makefnmark}{\mbox{\normalfont\@thefnmark})}
\settowidth{\footnotemargin}{***) }
\patchcmd{\@makefntext}{\hss\@makefnmark}{\hss \@makefnmark\ }{}{}
\makeatother

%Line Spacing
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\footnotelayout}{ \baselineskip=1.02\baselineskip }
\setlength{\skip\footins}{\baselineskip}

\theoremstyle{plain}
\newtheorem*{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{problem}{Problem}


\theoremstyle{remark}
\newtheorem*{example}{Example}
\newtheorem*{examples}{Examples}

\begin{document}
\pagenumbering{gobble}
\setlength{\abovedisplayskip}{0.5\baselineskip}
\setlength{\belowdisplayskip}{0.5\baselineskip}


\section*{\hspace{1pt} \\[3\baselineskip]
NEW PROOF OF THE THEOREM THAT  \\[\baselineskip]
\begin{large}EVERY ALGEBRAIC RATIONAL FUNCTION\end{large} \\[\baselineskip]
OF ONE VARIABLE \\[\baselineskip]
\begin{small}CAN BE RESOLVED INTO REAL FACTORS OF FIRST OR SECOND DEGREE.\end{small}\\[\baselineskip]
\rule{0.85in}{0.5pt}}

\subsection*{1.}

Any given algebraic equation can be reduced to the form \[ x^m + Ax^{m-1} + Bx^{m-2} + \text{etc.} + M = 0 \] such that \(m\) is a positive integer. If we denote the first part of this equation by \(X\), and assume that the equation \(X = 0\) is satisfied by multiple unequal values of \(x\), say by setting \(x=\alpha\), \(x =\beta\), \(x = \gamma\), etc., then the function \(X\) will be divisible by the product of the factors \(x-\alpha\), \(x-\beta\), \(x-\gamma\), etc. Conversely, if the product of several simple factors \(x-\alpha\), \(x-\beta\), \(x-\gamma\), etc. divides the function \(X\), the equation \(X = 0\) will be satisfied by setting \(x\) equal to any of the quantities \(\alpha, \beta, \gamma\), etc. Finally, if \(X\) is equal to the product of \(m\) such simple factors (whether they are all different or some of them are identical), then no other simple factors besides these can divide the function \(X\). Therefore, an equation of degree \(m\) cannot have more roots than \(m\), but it is evident that an equation of degree \(m\) can have \textit{fewer} roots, even if \(X\) is resolvable into \(m\) simple factors: if some of these factors are identical, then the number of different ways the equation can be satisfied will necessarily be less than \(m\). Nevertheless, for the sake of elegance, geometers prefer to say that the equation has \(m\) roots even in this case, but that some of them are equal to each other: a liberty they could certainly take.

\subsection*{2.}

The things explained so far are sufficiently demonstrated in algebraic works and do not violate geometric rigor anywhere. However, analysts seem to have adopted the theorem on which almost the entire theory of equations is built too hastily and without proper solid proof: \textit{that a function such as \(X\) can always be resolved into \(m\) simple factors}, or, which is entirely consistent with it, \textit{that an equation of degree \(m\) does indeed have \(m\) roots}. But since in quadratic equations, we often encounter cases that contradict this theorem, algebraists were forced to introduce a certain imaginary quantity whose square is \(-1\), and then they acknowledged that if quantities of the form \(a+b\surd{-1}\) are treated as real, then the theorem holds not only for quadratic but also for cubic and biquadratic equations. However, it is by no means permissible to infer from this that any equation of the fifth degree or higher can be satisfied by admitting quantities of the form \(a+b\surd{-1}\), or as it is often expressed (although I would prefer a less slippery phrase), that the roots of such an equation can be reduced to the form \(a+b\surd{-1}\). This theorem, as stated in the title of this paper, does not differ from what has been mentioned before. The aim of this dissertation is to provide a new rigorous demonstration of this theorem.

Moreover, since analysts discovered that there are infinitely many equations that have no roots at all unless quantities of the form \(a+b\surd{-1}\) are admitted, these fictitious quantities, considered as a special kind of quantity and called imaginary to distinguish them from real ones, have been introduced into the entire analysis; on what grounds? I do not dispute this point here.  I will complete my demonstration without any aid from imaginary quantities, although I would be allowed the same freedom that all recent analysts have used.

 \subsection*{3.}

Although what is presented in most elementary books as the proof of our theorem is so trivial and deviates so much from geometric rigor that it is scarcely worth mentioning, I will touch on it briefly so that nothing seems to be lacking. To demonstrate that the equation \[x^m + Ax^{m-1} + Bx^{m-2} + \text{etc.} + M = 0\] or \(X = 0\) indeed has \(m\) roots, they attempt to prove that \(X\) can be resolved into \(m\) simple factors. To achieve this, they assume \(m\) simple factors \(x-\alpha\), \(x-\beta\), \(x-\gamma\), etc., where \(\alpha\), \(\beta\), \(\gamma\), etc. are still unknown, and they set the product of these equal to the function \(X\). Then, by comparing coefficients, they deduce \(m\) equations from which they claim that the unknowns \(\alpha\), \(\beta\), \(\gamma\), etc. can be determined, and the number of these equations is also \(m\). In particular, \(m-1\) unknowns can be eliminated, leading to an equation that contains only the unknown \(\alpha\). Leaving aside other criticisms that could be made of this argumentation, let us simply ask how we can be certain that the final equation indeed has any roots.  Why couldn't it be the case that neither this final equation nor any magnitude proposed satisfies any value in the entire range of real and imaginary quantities?  However, experts will easily see that this final equation must necessarily be \textit{entirely identical} to the proposed one if the calculation is properly conducted; namely, after eliminating the unknowns \(\beta\), \(\gamma\), etc., the equation \[\alpha^m + A\alpha^{m-1} + B\alpha^{m-2} + \text{etc.} + M = 0\] should emerge. There is no need to elaborate further on this reasoning.

Some authors, who seem to have perceived the weakness of this method, take it as an \textit{axiom} that any equation indeed has roots, whether possible or impossible. However, they do not seem to have clearly explained what is meant by possible and impossible quantities. If possible quantities are meant to denote the same as real and impossible as imaginary, this axiom cannot be admitted without proper demonstration, and instead requires proof. Nevertheless, the terms do not seem to be intended in that sense, but rather the meaning of the axiom appears to be: `Although we are not yet certain that there necessarily exist \(m\) real or imaginary quantities satisfying a given equation of degree \(m\), we will assume it for a while; for if by chance it should happen that so many real and imaginary quantities cannot be found, then at least we have an escape, and we can say that the remaining ones are impossible.' If someone prefers to use this phrase rather than simply saying that the equation in this case will not have so many roots, I have no objection; but if, at that point, they treat these impossible roots as if they were something true, and, for example, say that the sum of all the roots of the equation \(x^m + A x^{m-1} + \text{etc.} = 0\) is \(= -A\), even if some of them are impossible (which expression explicitly means \textit{even if some are missing}), then I cannot approve of it. For impossible roots, accepted in this sense, are still roots, and then that axiom cannot be admitted without some kind of demonstration, as it is not unreasonable to ask whether equations can exist that do not even have impossible roots.\footnote{I always understand the term imaginary quantity here to refer to a quantity in the form \(a+b\surd{-1}\) as long as \(b\) is not equal to \(0\). In this sense, this expression has always been accepted by all geometers of the first order, and I consider those who wanted to call the quantity \(a+b\surd{-1}\) imaginary only in the case where \(a = 0\), and impossible only when \(a \neq 0\), not worth listening to, as this distinction is neither necessary nor of any utility.  If imaginary quantities are to be retained in analysis altogether (which seems more advisable than abolishing them, provided they are solidly established), then they must necessarily be regarded as equally possible as real quantities; hence, I would prefer to encompass real and imaginary quantities under the common designation of \textit{possible quantities}: conversely, I would call a quantity \textit{impossible} if it should satisfy conditions that cannot be satisfied even by admitting imaginary ones, yet in a way that this \textit{phrase} means the same as saying that such a quantity does not exist in the entire range of magnitudes. From this standpoint, I would not concede the formation of a peculiar class of quantities. If someone says that an equilateral right-angled triangle is impossible, no one will deny it. But if he wants to consider such an impossible triangle as a new kind of triangles and apply properties of other triangles to it, would anyone take it seriously? This would be playing with words or rather abusing them. Although even eminent mathematicians have often applied truths that manifestly presuppose the possibility of certain quantities to cases where the possibility was still doubtful; and while I do not deny that such licenses usually pertain only to the form and semblance of reasoning, which the keen edge of true geometry can soon penetrate: yet it seems more advisable and more worthy of the sublimity of a science celebrated as the most perfect example of clarity and certainty, either to entirely prohibit such liberties, or at least to use them sparingly and only where those less practiced might find it difficult to perceive the matter without their aid, and where it could still be handled as rigorously, if perhaps less briefly.  However, I do not deny that what I have said here against the abuse of impossibilities can be applied in some respects against imaginaries as well: yet I reserve the vindication of these and the fuller exposition of this whole matter for another occasion.}

\subsection*{4.}

Before I review the demonstrations of our theorem by other geometers and point out what seems to me to be objectionable in each, I make the following observation: it is sufficient to show only that for any equation of any degree \[x^m + Ax^{m-1} + Bx^{m-2} + \text{etc.} + M = 0\]
or \(X = 0\) (where the coefficients \(A, B\) etc. are assumed to be real), it can be satisfied in at least one way by the value of \(x\) expressed as \(a+b\surd{-1}\). For it is evident that \(X\) will then be divisible by a real quadratic factor \(xx - 2ax + aa + bb\) if \(b\) is not \(=0\), and by a real simple factor \(x-a\) if \(b = 0\). In both cases, the factor will be real and of lower degree than \(X\); and since, by the same reasoning, it must have a real factor of the first or second degree, it is clear that by continuing this operation, the function \(X\) will eventually be resolved into real simple or double factors, or, if you prefer to use two simple imaginaries instead of each real double factor, into \(m\) simple factors.

\subsection*{5.}

The first proof of the theorem is owed to the illustrious geometer \textsc{d'Alembert} and can be found in \textit{Recherches sur le calcul integral, Histoire de l'Acad. de Berlin, Année} 1746, p. 182 ff. The same proof is also in \textsc{Bougainville}, \textit{Traité du calcul integral, à Paris} 1754, p. 47 ff. The main points of his method are as follows.

First, he shows that if any function \(X\) of the variable \(x\) becomes \(= 0\) either for \(x = 0\) or for \(x = \infty\), and if it can acquire an infinitely small positive real value by assigning a real value to \(x\), then this function can also obtain an infinitely small negative real value, either from a real value of \(x\), or an imaginary value of the form \(p+q\surd{-1}\).  Namely, denoting \(\Omega\) as the infinitely small value of \(X\), and \(\omega\) as the corresponding value of \(x\), he asserts that \(\omega\) can be expressed by a highly convergent series \(a\Omega^{\alpha} + b \Omega^{\beta} + c \Omega^{\gamma} \) etc., where the exponents \(\alpha\), \(\beta\), \(\gamma\) etc. are continuously increasing rational quantities, and thus become at least a certain distance from the positive starting point and make the terms in which they appear infinitely small. Now, if among all these exponents there is none that is a fraction with an even denominator, all the terms of the series become real for both positive and negative values of \(\Omega\); but if some fractions with even denominators are found among these exponents, it is established that, for negative values of \(\Omega\), the corresponding terms are expressed in the form \(p+q\surd{-1}\). However, due to the convergence of the infinite series in the former case, it suffices to retain only the first (i.e., the largest) term; in the latter case, it is unnecessary to go beyond the term that first introduces an imaginary part.

Through similar reasoning, it can be shown that if \(X\) can obtain a negative infinitely small value from a real value of \(x\), then that function can acquire a positive real infinitely small value from a real value of \(x\) or from an imaginary value in the form \(p+q\surd{-1}\).

Hence he further concludes that a real finite value of \(X\) can also be found, in the former case negative and in the latter case positive, which can be produced from an imaginary value of \(x\) of the form \(p+q\surd{-1}\).

From this, it follows that if \(X\) is a function of \(x\) such that it obtains a real value \(V\) from a real value \(v\) of \(x\), and also obtains an infinitely small real value, either greater or smaller than \(V\), from a real value of \(x\), then it can also receive an infinitely small real value, which is either smaller and greater than \(V\) (respectively), by assigning to \(x\) a value of the form \(p+q\surd{-1}\). This is easily derived from the above if \(X\) is conceived to be replaced by \(V + Y\), and \(x\) by \(v+y\).

Finally, \textsc{d'Alembert} asserts that \(X\) can traverse any interval between two real values \(R\), \(S\) (i.e., becoming equal to \(R\) and \(S\), and all intermediate real values), by assigning to \(x\) values of the form \(p+q\surd{-1}\); that is, the function \(X\) can increase or decrease by any finite real quantity (depending on whether \(S>R\) or \(S<R\)), while \(x\) remains always in the form \(p+q\surd{-1}\). For if a real quantity \(U\) is given (which is supposed to lie between \(R\) and \(S\)), such that \(X\) cannot be made equal to \(U\) by such a value of \(x\), then necessarily a \textit{maximum} value of \(X\) must be given (when \(S>R\); and a minimum when \(S<R\)), say \(T\), which would be obtained from a value of \(x\), \(p+q\surd{-1}\), in such a way that no value of \(x\) in a similar form could be assigned that would bring the function \(X\) closer to \(U\) by the smallest excess. Now, if in the equation between \(X\) and \(x\), \(p+q\surd{-1}\) is substituted everywhere for \(x\), and then the real part and the part involving the factor \(\surd{-1}\) are equated, two equations will result from this (in which \(p\), \(q\), and \(X\) occur mixed with constants) that, through elimination, can yield two others, in one of which \(p\), \(X\), and constants are found, and in the other, \(p\) is free containing only \(q\), \(X\), and constants. Therefore, since \(X\) has traversed all values from \(R\) to \(T\) by real values of \(p, q\), according to the above, \(X\) can still approach the value \(U\) by assigning values to \(p\), \(q\) such as \(\alpha + \gamma \surd{-1}\), \(\beta + \delta\surd{-1}\), respectively. From this, it follows that \(x = \alpha - \delta + (\gamma + \beta)\surd{-1}\), i.e. it is still in the form \(p+q\surd{-1}\), contrary to the hypothesis.


Now, if \(X\) is supposed to represent a function such as \(x^m + Ax^{m-1} + Bx^{m-2} + \text{etc.} + M\), it is clear that there is no problem, and such real values can be assigned to \(x\) so that \(X\) traverses any interval between two real values. Therefore, \(x\) can also obtain some value in the form \(p+q\surd{-1}\), from which \(X\) becomes \(=0\). Q.E.D.\footnote{It is worth noting that \textsc{d'Alembert} in his exposition of this demonstration used geometric considerations, viewing \(X\) as the abscissa and \(x\) as the ordinate of the curve (in the manner of all geometers of the first part of this century, among whom the notion of functions was less common). However, since all his reasoning, if you consider only their essence, relies on purely analytic principles, and imaginary curves and expressions of imaginary ordinates may seem harder and more likely to confuse the modern reader, I preferred to use a purely analytic form of representation here. I added this note to prevent anyone from suspecting that something essential had been changed by comparing \textsc{d'Alembert}'s demonstration itself with this concise exposition.}

\subsection*{6.}

The objections that can be raised against \textsc{d'Alembert}'s demonstration mostly come down to the following.

1. \textsc{d'Alembert} raises no doubt about the \textit{existence} of values of \(x\) for which the given values of \(X\) correspond but assumes it and only investigates the \textit{form} of these values.

Although this objection is in itself very serious, it pertains here only to the form of expression, which can easily be corrected to completely invalidate it.

2. The assertion that \(\omega\) can always be expressed by such a series as he posits is certainly false if \(X\) is supposed to represent any transcendental function (as \textsc{d'A.} hints at in several places). This is evident, for example, if \(X = e^{\frac{1}{x}}\) or \(X = \frac{1}{\log X}\). However, if we restrict the demonstration to the case where \(X\) is an algebraic function of \(x\) (which is sufficient for the present matter), the proposition is certainly true. Nevertheless, \textsc{d'A.} provided no evidence to support his assumption; the illustrious \textsc{Bougainville} assumes that \(X\) is an algebraic function of \(x\) and recommends the use of \textsc{Newton}'s parallelogram series for finding it.

3. He uses infinitely small quantities more freely than can be justified with geometric rigor or at least would be granted by a careful analyst in our age (where they rightly face skepticism). He also did not explain the leap from the value of \(\Omega\) being infinitely small to it being finite sufficiently clearly. His conclusion that \(\Omega\) can obtain a finite value seems to be derived not so much from the possibility of an infinitely small value of \(\Omega\) as from the fact that, denoting \(\Omega\) as a very small quantity, due to the great convergence of the series, the closer the true value of \(\omega\) is approached, or the more accurately the equation expressing the relation between \(\omega\) and \(\Omega\) or \(x\) and \(X\) is satisfied, the more terms of the series are taken. Furthermore, the entire argument seems too vague to draw any rigorous conclusions from it: it should be noted that there are series that, no matter how small a value is assigned to the quantity according to which their powers progress, always diverge, so that if they continue far enough, you can reach terms greater than any given quantity\footnote{By the way, on this occasion, I note incidentally that there are many series that initially seem to converge greatly, most notably those used by \textsc{Euler} in the latter part of \textit{Institutiones Calculi Differentialis} Chapter VI. for approximating the sum of other series (for the remaining series on p. 475-478 can indeed converge), which, as far as I know, has not been noticed by anyone so far. Therefore, it would be highly desirable to clearly and rigorously demonstrate why such series, which converge very quickly at first, then more slowly, and finally more and more slowly, nevertheless provide an approximation to the true sum, as long as not too many terms are taken, and until such a sum can be safely considered exact.}. This happens when the coefficients of the series constitute a hypergeometric progression. Therefore, it should have been necessarily demonstrated that such a hypergeometric series cannot arise in the present case.

However, it seems to me that \textsc{d'A.} did not rightly resort to infinite series here, and they are not suitable for establishing this fundamental theorem of the theory of equations.

 4. From the assumption that \(X\) can attain the value \(S\) but not the value \(U\), it does not necessarily follow that there must be a value \(T\) between \(S\) and \(U\) which \(X\) can reach but not exceed. Another case remains: namely, it could be that there is a limit between \(S\) and \(U\) that can be approached as closely as desired by \(X\), but \(X\) never actually reaches it. From the arguments provided by \textsc{d'A.}, it only follows that \(X\) can always surpass any value it has reached by a finite quantity, for example, when it has become \(= S\), it can still increase by some finite quantity \(\Omega\), and with this, a new increment \(\Omega'\) may occur, then another increase \(\Omega''\), etc., so that no increment should be considered final, but there can always be a new one added. Although the multitude of possible increments is not limited by any boundaries, it could still be the case that if the increments \(\Omega\), \(\Omega'\), \(\Omega''\), etc. continuously decrease, the sum \(S + \Omega + \Omega' + \Omega''\), etc. never reaches a certain limit, no matter how many terms are considered.

While this case cannot occur when \(X\) represents a complete algebraic function of \(x\), without a demonstration, this inability to occur must necessarily be considered a methodological flaw. However, when \(X\) is a transcendental function or even a fractional algebraic function, this case can indeed occur, for example, whenever a certain value of \(X\) corresponds to an infinitely large value of \(x\). Then, the \textsc{d'Alembert}ian method seems not without many difficulties and possibly in some cases impossible to reduce to unquestionable principles.

For these reasons, I cannot consider the \textsc{d'Alembert}ian demonstration as satisfactory. However, despite this, I do not believe that the true essence of the demonstration is in any way compromised by all objections, and I think that based on the same foundation (though with a vastly different rationale and at least a more comprehensive perspective), not only a rigorous demonstration of our theorem can be built, but also everything that can be desired concerning the theory of \textit{transcendent} equations. I will discuss this matter more extensively on another occasion; see meanwhile below, article 24.

 \subsection*{7.}

After \textsc{d'Alembert}, \textsc{Euler} published his investigations on the same subject in \textit{Recherches sur les racines imaginaires des equations, Hist. de l'Acad. de Berlin A.} 1749, p. 223 sqq. He presented two methods, and the essence of the first is summarized in the following.

First, \textsc{Euler} aims to demonstrate that if \(m\) is any power of \(2\), then the function \(x^{2m} + Bx^{2m-2} + C x^{2m-3} + \text{etc.} + M = X\) (where the coefficient of the second term is \(= 0\)) can always be resolved into two real factors, in which \(x\) has up to \(m\) dimensions. To achieve this, he considers two factors: \[x^m - u x^{m-1} + \alpha x^{m-2} + \beta x^{m-3} + \text{etc., and } x^m + u x^{m-1}+\lambda x^{m-2} + \mu x^{m-3} + \text{etc.}\] where the coefficients \(u\), \(\alpha\), \(\beta\), etc., \(\lambda\), \(\mu\), etc. are still unknown. Their product is set equal to the function \(X\). The comparison of coefficients yields \(2m-1\) equations, and it only needs to be shown that the unknowns \(u\), \(\alpha\), \(\beta\), etc., \(\lambda\), \(\mu\), etc. (whose number is also \(2m -1\)) can be assigned real values satisfying these equations. \textsc{Euler} asserts that if \(u\) is considered as known initially, such that the number of unknowns is one less than the number of equations, then by properly combining these using algebraic methods, all \(\alpha\), \(\beta\), etc., \(\lambda\), \(\mu\), etc. can be rationally determined, without any extraction of roots, by \(u\) and the known coefficients \(B, C\), etc. Furthermore, all \(\alpha\), \(\beta\), etc., \(\lambda\), \(\mu\), etc. can be eliminated, resulting in the equation \(U = 0\), where \(U\) is an integral function of \(u\) and the known coefficients. It suffices here to know one property of this equation, namely, that the last term in \(U\) (which does not involve the unknown \(u\)) must be negative. It follows that the equation must have at least one real root, meaning that \(u\) and consequently \(\alpha\), \(\beta\), etc., \(\lambda\), \(\mu\), etc. can be determined in at least one real way. This property can be confirmed through the following reflections: When \(x^m - u x^{m-1}+\alpha x^{m-2} +\) etc. is assumed to be a factor of the function \(X\), \(u\) will necessarily be the sum of \(m\) roots of the equation \(X = 0\), and thus it must have as many values as there are ways to choose \(m\) out of \(2m\) roots, which is given by the combinatorial calculation \(\frac{2m . 2m-1 . 2m-2 . \dots . m+1}{1.2.3 \dots m}\). This number will always be oddly even (a not difficult demonstration is omitted here): if \(2k\) is assumed for this number, then \(k\) will be odd; the equation \(U = 0\) will then be of the \(2k^{th}\) degree. Now, since the second term is missing in the equation \(X = 0\), the sum of all \(2m\) roots will be \(0\). It is clear that if the sum of any \(m\) roots is \(+p\), the sum of the remaining roots will be \(-p\), i.e., if \(+p\) is among the values of \(u\), then \(-p\) will also be among them. Hence, \textsc{Euler} concludes that \(U\) is the product of \(k\) double factors of the form \(uu - pp\), \(uu - qq\), \(uu - rr\), etc., representing \(+p\), \(-p\), \(+q\), \(-q\), etc., all \(2k\) roots of the equation \(U = 0\). Therefore, due to the multitude of odd factors, the last term in \(U\) will be the square of the product \(pqr\) etc., with a negative sign. Moreover, the square of this product can always be rationally determined from the coefficients \(B, C\), etc., and will consequently be a real quantity. Thus, the square of this with a negative sign will certainly be a negative quantity. Q.E.D.

Since these two real factors of \(X\) are of degree \(m\), and \(m\) is a power of the number \(2\), each factor can again be resolved into two real factors of dimension \(\tfrac{1}{2}m\), by the same reasoning. However, as through repeated halving of the number \(m\), one necessarily eventually reaches two, it is evident that by the continuation of this operation, the function \(X\) will ultimately be resolved into real second-degree factors.

If, on the other hand, a function is presented in which the second term is not lacking, say \(x^{2m} + A x^{2m-1} + B x^{2m-2} + \text{etc.} + M\), also denoting \(2m\) as the power of binary, this will, by the substitution \(x = y - \frac{A}{2m}\), be transformed into a similar function lacking a second term. Hence, it is easily concluded that such a function is also resolvable into real second-degree factors.

Finally, for a given function of degree \(n\), where \(n\) is not a binary power: let the nearest higher binary power be denoted as \(= 2m\), and multiply the given function by \(2 m - n\) arbitrary real simple factors. From the resolvability of the product into real second-degree factors, it is straightforwardly derived that the given function must also be resolvable into real factors of the second or first degree.

\subsection*{8.}

Against this demonstration, one can object:

1. The rule by which \textsc{E.} concludes that from \(2m-1\) equations, \(2m-2\) unknowns \(\alpha\), \(\beta\) etc., \(\lambda\), \(\mu\) etc. can all be rationally determined, is not general but often admits exceptions. For example, if in Art. 3 one attempts to express the remaining unknowns and coefficients rationally by considering some unknowns as known, they will easily find that this is impossible, and that the unknown quantities can only be determined by an equation of degree \((m-1)^{ti}\). Although it can be immediately seen a priori that this must necessarily happen, it could be rightly doubted whether, even in the present case, for certain values of \(m\), the situation is such that the unknowns \(\alpha\), \(\beta\) etc., \(\lambda\), \(\mu\) etc. cannot be determined by an equation possibly of a degree greater than \(2m\). For the case where the equation \(X = 0\) is of the fourth degree, \textsc{E.} extracts rational values of the coefficients through \(u\) and the given coefficients; the same can indeed be done in all higher-degree equations, but it certainly requires a more extensive explanation. However, it seems worthwhile to delve more deeply and more generally into those formulas that rationally express \(\alpha\), \(\beta\) etc. through \(u, B, C\) etc.; I will undertake a more detailed discussion on this and other matters related to the theory of elimination (an argument by no means exhausted) on another occasion.

2. However, even if it is demonstrated that for an equation of any degree \(X = 0\), formulas can always be found that express \(\alpha\), \(\beta\), etc., \(\lambda\), \(\mu\), etc. rationally through \(u\), \(B\), \(C\), etc., it is certain that for certain specific values of the coefficients \(B\), \(C\), etc., those formulas can become \textit{indeterminate}, so that not only is it impossible to define those unknowns rationally from \(u\), \(B\), \(C\), etc., but in some cases, no real values of \(\alpha\), \(\beta\), etc., \(\lambda\), \(\mu\), etc. correspond to any real value of \(u\). For the confirmation of this matter, for brevity, I refer the reader to \textsc{E.}'s dissertation itself, where on p. 236 the equation of the fourth degree is more extensively explained. Everyone will immediately see that the formulas for the coefficients \(\alpha\), \(\beta\) become indeterminate if \(C = 0\) and the value \(0\) is assumed for \(u\), and their values cannot be assigned without extracting roots, and even more, not real values, if the quantity \(BB-4D\) is negative. Although in this case it is easy to see that \(u\) can still have other real values for which real values of \(\alpha\), \(\beta\) correspond, still, someone might fear that the solution of this difficulty (which \textsc{E.} did not touch at all) may require much more effort in higher-degree equations. Certainly, this matter should by no means be passed over in silence in an exact demonstration.

3. The illustrious \textsc{E.} tacitly assumes that the equation \(X=0\) has \(2m\) roots, and he establishes that their sum is \(= 0\) because the second term in \(X\) is absent. My opinion on this assumption (which all authors use in this argument) was already declared in Art. 3 above. The proposition that the sum of all roots of an equation equals the negative of the coefficient of the first term does not seem to be applicable to equations except those which have roots. Since it must be proved by this very demonstration that the equation \(X=0\) actually has roots, it does not seem permissible to assume their existence. Undoubtedly, those who have not yet penetrated the fallacy of this paralogism will respond, \textit{it is not demonstrated here that the equation \(X=0\) can be satisfied} (for this expression means having roots), \textit{but that it can only be satisfied by values of \(x\) of the form \(a + b \surd{-1}\); the former is to be taken as an axiom}. However, since other forms of quantities cannot be conceived beyond the real and imaginary \(a + b \surd{-1}\), it is not clear enough how what should be demonstrated differs from what is assumed as an axiom; indeed, even if it were possible to conceive other forms of quantities, such as \(F\), \(F'\), \(F''\), etc., it should not be admitted without demonstration that the equation can be satisfied by some value of \(x\) either real, or in the form \(a + b \surd{-1}\), or in the form \(F\), or in \(F'\), etc. Therefore, that axiom cannot have any other meaning than this: Any equation can be satisfied \textit{either} by a real value of the unknown, \textit{or} by an imaginary value in the form \(a + b \surd{-1}\), \textit{or} perhaps by a value in some hitherto unknown form, \textit{or} by a value that is not contained in any form whatsoever. But how such quantities, about which one cannot even imagine an idea — true shadows of shadows — can be added or multiplied, is not understood with the clarity demanded in mathematics\footnote{All of this will be much elucidated by another dissertation already sweating under the press, where in a completely different argument, but nevertheless analogous, I could have used a similar license with exactly the same right, as has been done here in equations by all analysts. Although the proofs of several truths could have been completed in a few words with the help of such fictions, which otherwise become very difficult and require the most subtle artifices, I preferred to abstain from them altogether and hoped to have satisfied a few if I followed the method of analysts.}.

Now, I do not intend to render the conclusions that \textsc{E.} derived from his assumption at all suspect through these objections; rather, I am confident that they can be confirmed by a method neither difficult nor very different from the \textsc{Euler}ian one, in such a way that there should be no doubt left for anyone, even the slightest. I only criticize the \textit{form}, which, although it can be of great utility in \textit{discovering} new truths, seems to be not at all commendable in \textit{demonstrating} before the public.

4. As for the demonstration of the assertion that the product \(pqr\) etc., can be rationally determined from the coefficients in \(X\), the illustrious \textsc{E.} has brought nothing at all. All that he explains on this matter in equations of \textit{fourth degree} is as follows (where \(\mathfrak{a}\), \(\mathfrak{b}\), \(\mathfrak{c}\), \(\mathfrak{d}\) are the roots of the proposed equation \(x^4 + B xx + C x + D = 0\)):

`On m'objectera sans doute, que j'ai suppos\'e ici, que la quantit\'e \(pqr\) etait une quantit\'e r\'eelle, et que son quarr\'e \(ppqqrr\) \'etait affirmatif; ce qui \'etait encore douteux, vu que les racines \(\mathfrak{a}\), \(\mathfrak{b}\), \(\mathfrak{c}\), \(\mathfrak{d}\) etant imaginaires, il pourrait bien arriver, que le quarr\'e de la quantit\'e \(pqr\), qui en est compos\'ee, fut n\'egatif. Or je r\'eponds \`a cela que ce cas ne saurait jamais avoir lieu; car quelque imaginaires que soient les racines \(\mathfrak{a}\), \(\mathfrak{b}\), \(\mathfrak{c}\), \(\mathfrak{d}\), on sait pourtant, qu'il doit y avoir \(\mathfrak{a} + \mathfrak{b} + \mathfrak{c} + \mathfrak{d} = 0 \); \(\mathfrak{a}\mathfrak{b} + \mathfrak{a}\mathfrak{c} + \mathfrak{a}\mathfrak{d} + \mathfrak{b}\mathfrak{c} + \mathfrak{b}\mathfrak{d} + \mathfrak{c}\mathfrak{d} = B\); \(\mathfrak{a}\mathfrak{b}\mathfrak{c}+\mathfrak{a}\mathfrak{b}\mathfrak{d}+\mathfrak{a}\mathfrak{c}\mathfrak{d}+\mathfrak{b}\mathfrak{c}\mathfrak{d} = -C\) \footnote{\textsc{E.} per errorem habet \(C\), unde etiam postea perperam statuit \(pqr = C\).}; \(\mathfrak{a}\mathfrak{b}\mathfrak{c}\mathfrak{d}=D\), ces quantites \(B\), \(C\), \(D\) \'etant r\'eelles. Mais puisque \(p = \mathfrak{a} + \mathfrak{b}\), \(q =\mathfrak{a} + \mathfrak{c}\), \(r = \mathfrak{a} + \mathfrak{d}\), leur produit \(pqr = (\mathfrak{a} + \mathfrak{b})(\mathfrak{a} + \mathfrak{c})(\mathfrak{a} + \mathfrak{d})\) est d\'eterminable \textit{comme on sait}, par les quantit\'es \(B\), \(C\), \(D\), et sera par cons\'equent r\'eel, tout comme nous avons vu, qu'il est effectivement \(pqr = -C\), et \(ppqqrr = CC\). On reconna\^{\i}tra ais\'ement de m\^eme, que dans les plus hautes \'equations cette m\^eme circonstance doit avoir lieu, et qu'on ne saurait me faire des objections de ce c\^ot\'e.' 

However, \textsc{E.} did not add anywhere that the product \(pqr\), etc., can be rationally determined by \(B\), \(C\), etc., although he seems to have always understood it implicitly, as without it the demonstration can have no force. Indeed, it is true in equations of the fourth degree that if the product \((\mathfrak{a} + \mathfrak{b})(\mathfrak{a} + \mathfrak{c})(\mathfrak{a} + \mathfrak{d})\) is expanded, it yields \(\mathfrak{a}\mathfrak{a}(\mathfrak{a} + \mathfrak{b} + \mathfrak{c} + \mathfrak{d}) \mathfrak{a}\mathfrak{b}\mathfrak{c}+\mathfrak{a}\mathfrak{b}\mathfrak{d}+\mathfrak{a}\mathfrak{c}\mathfrak{d}+\mathfrak{b}\mathfrak{c}\mathfrak{d}= - C\). However, it does not seem clear enough how, in all higher-degree equations, the product can be rationally determined by the coefficients. The distinguished \textsc{de Foncenex}, who first observed this (\textit{Miscell. phil. math. soc. Taurin. Vol. I, p. 117}), rightly contends that without a rigorous demonstration of this proposition, the method loses all its force, and he admits that it seems quite difficult to him, describing the fruitless attempts he made in that direction\footnote{An error seems to have crept into this explanation, namely on p. 118, line 5. Instead of "characteris (on choisissait seulement Celles oü entrait p etc.)," one must necessarily read "une même racine quelconque de l'équation in-oposee," or something similar, as the former has no meaning.}. However, this matter can be easily completed by the following method (of which I can only provide a summary here): Although it is not clear enough in equations of the fourth degree that the product \((\mathfrak{a} + \mathfrak{b})(\mathfrak{a} + \mathfrak{c})(\mathfrak{a} + \mathfrak{d})\) can be determined by the coefficients \(B\), \(C\), \(D\), it can be easily seen that the same product is also \(= (\mathfrak{b} + \mathfrak{a})(\mathfrak{b} + \mathfrak{c})(\mathfrak{b} + \mathfrak{d})\), as well as \( = (\mathfrak{c} + \mathfrak{a})(\mathfrak{c} + \mathfrak{b})(\mathfrak{c} + \mathfrak{d})\), and finally also \( = (\mathfrak{d} + \mathfrak{a})(\mathfrak{d} + \mathfrak{b})(\mathfrak{d} + \mathfrak{c})\). Therefore, the product \(pqr\) will be a quarter of the sum \((\mathfrak{b} + \mathfrak{a})(\mathfrak{b} + \mathfrak{c})(\mathfrak{b} + \mathfrak{d})+(\mathfrak{c} + \mathfrak{a})(\mathfrak{c} + \mathfrak{b})(\mathfrak{c} + \mathfrak{d})+(\mathfrak{d} + \mathfrak{a})(\mathfrak{d} + \mathfrak{b})(\mathfrak{d} + \mathfrak{c})\), which, if expanded, can be foreseen a priori to be a rational integral function of the roots \(\mathfrak{a}\), \(\mathfrak{b}\), \(\mathfrak{c}\), \(\mathfrak{d}\) in which they all enter in the same way. Such functions can always be expressed rationally by the coefficients of the equation whose roots are \(\mathfrak{a}\), \(\mathfrak{b}\), \(\mathfrak{c}\), \(\mathfrak{d}\). — The same is also evident if the product \(pqr\) is brought into this form: \[ \tfrac{1}{2}\left( \mathfrak{a} + \mathfrak{b} - \mathfrak{c} - \mathfrak{d}  \right) \times  \tfrac{1}{2}\left( \mathfrak{a} + \mathfrak{c} - \mathfrak{b} - \mathfrak{d}  \right)  \times  \tfrac{1}{2}\left( \mathfrak{a} + \mathfrak{d} - \mathfrak{b} - \mathfrak{c}  \right)  \]

The expanded product of this expression, involving all \(\mathfrak{a}\), \(\mathfrak{b}\), \(\mathfrak{c}\), \(\mathfrak{d}\) in the same way, can be easily foreseen. Knowledgeable individuals will simultaneously gather how this can be applied to higher-degree equations. I reserve the complete exposition of the demonstration, which brevity does not permit me to include here, along with a more extensive discussion of functions involving multiple variables, for another occasion.

Now, I observe that in addition to these four objections, there are still some other aspects in the demonstration of \textsc{E.} that could be criticized, which I pass over in silence lest I seem to be an overly severe critic, especially since the foregoing seems to sufficiently demonstrate that the demonstration, in the form in which it is proposed by \textsc{E.}, cannot be considered complete.

After this demonstration, \textsc{E.} presents another way to reduce the theorem for equations whose degree is not a binary power to the resolution of such equations. However, since this method teaches nothing for equations whose degree is a binary power and, moreover, is equally susceptible to all the aforementioned objections (except the fourth) as the initial general demonstration, there is no need to elaborate on it here.

\subsection*{9.}

In the same paper, on page 263, the illustrious \textsc{E.} endeavored to further confirm our theorem by another method, the essence of which is as follows: Given an equation \(x^n + Ax^{n-1} + B x^{n-2} \text{ etc.} = 0\), an analytic expression representing its roots explicitly could not be found so far for exponents \(n>4\); however, it seems certain (as \textsc{E.} asserts) that it can contain nothing else but arithmetic operations and root extractions, increasingly complicated as \(n\) grows. If this is conceded, \textsc{E.} excellently demonstrates that, no matter how complicated the radical signs are among themselves, the formulas can always be represented by the form \(M+N\surd{-1}\), where \(M, N\) are real quantities.

Against this reasoning, one can object that, after so many great efforts by geometers, there remains little hope of ever reaching a general solution for algebraic equations. It becomes more and more likely that such a resolution is entirely impossible and contradictory. This should not seem too paradoxical, especially since what is commonly called the solution of an equation is properly nothing other than its reduction to pure equations. For the solution of pure equations is not taught but assumed, and if you express the root of the equation \(x^m = H\) as \(\sqrt[m]{H}\), you have not solved it, nor have you done more than if you were to invent some symbol to denote the root of the equation \(x^n + Ax^{n-1}+ \text{ etc.} = 0\) and equate the root to it. It is true that pure equations, due to the ease of finding their roots by approximation and the elegant connection that all roots have with each other, excel above all others and are therefore not to be blamed for analysts denoting these roots by a specific symbol. However, it does not follow from this that the root of any equation can be expressed by these symbols. Or, in other words, it is assumed without sufficient reason that the solution of any equation can be reduced to the solution of pure equations. Perhaps it would not be so difficult to rigorously demonstrate the impossibility already for the fifth degree, about which I will present more extensive discussions elsewhere. Here, it suffices to note that the general solvability of equations, in the sense accepted here, is still highly doubtful, and therefore, the demonstration, whose entire validity depends on that assumption, currently carries no weight.

\subsection*{10.}

Later, the distinguished \textsc{de Foncenex}, having noticed a deficiency in Euler's initial demonstration (see objection 4 in article 8) that he could not rectify, attempted another approach, which he presented in the aforementioned commentary on page 120\footnote{Explanations related to this commentary are found in the second volume of the same Miscellanea on p. 337. However, these are not relevant to the current discussion but pertain to the logarithms of negative quantities, which were discussed in the same work.}. This approach is as follows:

Suppose we have the equation \(Z = 0\), representing a function of degree \(m\) in an unknown \(z\). If \(m\) is an odd number, then it is clear that this equation has a real root. However, if \(m\) is even, the distinguished \textsc{Foncenex} attempts to prove in the following way that the equation has at least one root of the form \(p + q\surd{-1}\). Let \(m = 2^n i\), where \(i\) is an odd number, and suppose that \(zz + uz + M\) is a divisor of the function \(Z\). Then each value of \(u\) will be the sum of two roots of the equation \(Z = 0\) (with the sign changed). Therefore, \(u\) will have \(\frac{m \cdot (m-1)}{1 \cdot 2} = m'\) values, and if \(u\) is assumed to be determined by the equation \(U = 0\) (where \(U\) is a function involving \(u\) and known coefficients in \(Z\)), this will be of degree \(m'\). It can be easily seen that \(m'\) will be of the form \(2^{n-1} i'\), where \(i'\) is an odd number. Now, unless \(m'\) is odd, assume again that \(uu + u'u + M'\) is a divisor of \(U\). By similar reasoning, \(u'\) will be determined by the equation \(U' = 0\), where \(U'\) is a function of degree \(\frac{m' \cdot (m'-1)}{1 \cdot 2}\) in \(u'\). Setting \(\frac{m' \cdot (m'-1)}{1 \cdot 2} = m''\), \(m''\) will be of the form \(2^{n-2}i''\), where \(i''\) is an odd number. Now, unless \(m''\) is odd, assume again that \(u'u'+u''u'+M''\) is a divisor of \(U'\), and then \(u''\) will be determined by the equation \(U'' = 0\), which has degree \(m'''\), where \(m'''\) is of the form \(2^{n-3}i'''\), an odd number. It is evident that in the series of equations \(U=0\), \(U'=0\), \(U''=0\), etc., the degree will be odd, and thus have a real root. For brevity, let us assume \(n = 3\), so that the equation \(U''=0\) has a real root \(u''\). It can be easily understood that the same reasoning holds for any other value of \(n\). Then, the coefficient \(M''\) and the coefficients in \(U'\) (which can be easily seen to be integral functions of the coefficients in \(Z\)) or are asserted by \textsc{de Foncenex} to be rationally determinable from \(u''\) and the coefficients of \(Z\), and are therefore real. It follows that the roots of the equation \(u'u'+u''u'+M''=0\) will be of the form \(p + q\surd{-1}\). They will also satisfy the equation \(U' = 0\), i.e., this equation will have roots of the form \(p + q\surd{-1}\). Finally, by similar reasoning, it follows that even \(M\) will be in the same form, and consequently, the root of the equation \(zz+uz+M=0\) will also satisfy the given equation \(Z=0\). Hence, any equation will have at least one root in the form \(p+q\surd{-1}\).

\subsection*{11.}

Objections 1, 2, 3, which I made against the first demonstration of \textsc{Euler} (art. 8), have the same force against this method. However, there is a difference, so that the second objection, to which \textsc{Euler's} demonstration was only liable in certain special cases, must now apply to all cases. Specifically, it can be a priori demonstrated that even if a formula is given expressing the coefficient \(M'\) rationally in terms of \(u\) and the coefficients in \(Z\), it must necessarily become indeterminate for multiple values of \(u'\); likewise, a formula expressing the coefficient \(M''\) in terms of \(u''\) must become indeterminate for certain values of \(u''\), and so on. This will be most clearly understood if we take the example of a quartic equation. Let us assume, therefore, that \(m = 4\), and let the roots of the equation \(Z = 0\) be \(\alpha\), \(\beta\), \(\gamma\), \(\delta\). Then it is clear that the equation \(U = 0\) will be of the sixth degree, and its roots will be \(-(\alpha+\beta)\), \(-(\alpha+\gamma)\), \(-(\alpha+\delta)\), \(-(\beta+\gamma)\), \(-(\beta+\delta)\), \(-(\gamma+\delta)\). The equation \(U'=0\) will be of the fifteenth degree, and its values of \(u'\) will be \[
\begin{array}{c}
\begin{array}{cccccc}
2\alpha+\beta+\gamma, & 2\alpha + \beta + \delta,& 2\alpha + \gamma + \delta, & 2\beta + \alpha + \gamma,& 2\beta + \alpha + \delta, & 2\beta + \gamma + \delta,\\
2\gamma+ \alpha + \beta, & 2\gamma + \alpha + \delta,& 2 \gamma + \beta + \delta,& 2 \delta + \alpha + \beta, &2\delta + \alpha + \gamma, &2\delta + \beta + \gamma,
\end{array} \\
\begin{array}{ccc}
\alpha + \beta + \gamma + \delta, & \alpha + \beta + \gamma + \delta, & \alpha + \beta + \gamma + \delta
\end{array}
\end{array}
\]
Now, in this equation, since its degree is odd, it will have to have a root, and it will indeed have the real root \(\alpha+\beta+\gamma+\delta\) (which, with the sign of the first coefficient in \(Z\) changed, is equal and therefore not only real but also rational, if the coefficients in \(Z\) are rational). But it can be easily seen that if a formula is given that rationally expresses the value of \(M'\) in terms of the corresponding value of \(u'\), it must necessarily become indeterminate for \(u' = \alpha + \beta + \gamma + \delta\). For this value is a root of the equation \(U' = 0\), and the three values of \(M'\) corresponding to it will be, for example, \((\alpha+\beta)(\gamma+\delta)\), \((\alpha+\gamma)(\beta+\delta)\), and \((\alpha+\delta)(\beta+\gamma)\), all of which can be irrational. Clearly, a rational formula could not produce an irrational value of \(M'\) in this case, nor could it produce three distinct values. From this example, it is evident that the method of \textsc{de Foncenex} is by no means satisfactory, but if it is to be made complete from every aspect, a much deeper investigation into the theory of elimination is required.

\subsection*{12.}

Finally, La Grange dealt with our theorem in his work \textit{Sur la forme des racines imaginaires des \'equations, Nouv. M\'em. de l'Acad. de Berlin 1772, p. 222 sqq}. This great geometer especially endeavored to supplement the deficiencies in Euler's first demonstration, particularly addressing those aspects constituting objections two and four as outlined above (art. 8). He delved so deeply into these matters that nothing more is desired, except perhaps in the previous discussion on the theory of elimination (on which this entire investigation is based), certain doubts may seem to remain. However, he did not touch upon the third objection at all, and the entire inquiry is built on the assumption that the equation of degree \(m\) indeed has \(m\) roots.

Therefore, with careful consideration of what has been presented so far, I hope that experts will find a new demonstration of this most important theorem, derived from entirely different principles, to be not unwelcome. I now proceed to present it.

\subsection*{13.}

\textsc{Lemma.} \textit{Let \(m\) denote any positive integer. Then the function \(\sin \varphi \cdot x^m - \sin m \varphi \cdot r^{m-1} x + \sin(m-1)\varphi \cdot r^m\) will be divisible by \(xx - 2 \cos \varphi \cdot rx + rr\)}.

\textit{Proof.} For \(m = 1\), the function becomes \(= 0\), and hence it is divisible by any factor. For \(m=2\), the quotient becomes \(\sin \varphi\), and for any larger value, it will be \( \sin \varphi.x^{m-2} + \sin 2\varphi . rx^{m-3} + \sin 3\varphi . rrx^{m-4} + \text{etc.}+\sin(m-1)\varphi . r^{m-2} \). It can be easily confirmed that by multiplying this function by \(xx-2\cos\varphi . rx + rr\), the product becomes equal to the given function.

\subsection*{14.}

\textsc{Lemma.} \textit{If the quantity \(r\) and the angle \(\varphi\) are determined in such a way that we have the equations
\begin{align*} r^m \cos m \varphi + A r^{m-1} \cos(m-1) \varphi + B r^{m-2}\cos(m-2) \varphi + \text{etc.} &\\
+ Krr \cos 2\varphi + Lr \cos \varphi + M &= 0  \tag*{[1]} \end{align*}
\begin{align*} r^m \sin m \varphi + A r^{m-1} \sin(m-1) \varphi + B r^{m-2}\sin(m-2) \varphi + \text{etc.}& \\
+ Krr \sin 2\varphi + Lr \sin \varphi & = 0  \tag*{[2]}\end{align*}
then the function \(x^m+Ax^{m-1} + Bx^{m-2}+\text{etc.}+Kxx + Lx + M = X\) will be divisible by the double factor \(xx-2\cos\varphi.rx+rr\), provided \(r\sin\varphi\) is not \(=0\); if \(r\sin\varphi = 0\), then the same function will be divisible by the simple factor \(x - r \cos\varphi\).}

\textit{Proof.} I. From the preceding article, all the following quantities will be divisible by \(xx - 2\cos \varphi . rx + rr\): \[\begin{array}{lll} \hspace{1em} \sin \varphi. rx^m &-\hspace{1em} \sin m \varphi . r^m x &+ \sin (m-1) \varphi . r^{m+1} \\ A\sin \varphi. rx^{m-1} &- A\sin (m-1) \varphi . r^{m-1} x &+ A\sin (m-2) \varphi . r^{m} \\ B\sin \varphi. rx^{m-2} &- B \sin (m-2) \varphi . r^{m-2} x &+ B\sin (m-3) \varphi . r^{m-1} \\ &\text{etc.} & \text{etc.} \\ K\sin \varphi. rx^2 &- K\sin 2 \varphi . rr x &+K\sin \varphi . r^{3} \\ L\sin \varphi. rx &- L \sin  \varphi . r x &  \\ M\sin \varphi. r &  &+ M\sin(-\varphi).r \end{array}\]

Therefore, the sum of these quantities will also be divisible by \(xx - 2\cos\varphi.rx + rr\). The terms of the first group constitute the sum \(\sin\varphi.rX\); the second group sums to \(0\) due to [2]; and it is easily seen that the sum of the third group also vanishes, if [1] is multiplied by \(\sin \varphi\) and [2] by \(\cos \varphi\), and the products are subtracted. Hence, it follows that the function \(\sin \varphi . r X\) is divisible by \(xx - 2 \cos \varphi .rx +rr\), and therefore, unless \(r \sin \varphi = 0\), the function \(X\) is also divisible. Q.E.P.

II. If \(r \sin \varphi = 0\), then either \(r = 0\) or \(\sin \varphi = 0\). In the former case, \(M=0\) due to [1], and therefore \(X\) is divisible by \(x\) or \(x - r\cos\varphi\); in the latter case, \(\cos \varphi = \pm 1\), \(\cos 2 \varphi = +1\), \(\cos 3 \varphi = \pm 1\), and generally \(\cos n\varphi = \cos \varphi^n\). Therefore, due to [1], \(X=0\) when \(x=r \cos \varphi\), and hence the function \(X\) is divisible by \(x-r\cos\varphi\). Q.E.S.

\subsection*{15.}

The preceding theorem is often demonstrated with the aid of imaginary quantities, see \textsc{Euler} \textit{Introductio in Analysin Infinitorum }Vol. I p.110; I deemed it worthwhile to show how it can be equally easily derived without their assistance. It is already evident that for the proof of our theorem, nothing else is required than to show: \textit{Given any function \(X\) of the form \( x^m + Ax^{m-1}+Bx^{m-2}+\text{etc.} +Lx + M\), \(r\) and \(\varphi\) can be determined in such a way that equations} [1] \textit{and} [2] \textit{hold}. From this, it will follow that \(X\) has a real factor of the first or second degree; however, the division will necessarily produce a real quotient of a lower degree, which, for the same reason, will also have a factor of the first or second degree. By continuing this operation, \(X\) will eventually be resolved into simple or double real factors. Thus, the goal of the following discussion is to prove that theorem.

\subsection*{16.}

Imagine an infinite fixed plane (the plane of the table, Fig. 1), and on this, an infinite fixed straight line \(GC\) passing through the fixed point \(C\). Assume any length as the unit so that all lines can be expressed by numbers.  At any point \(P\) on the plane, with a distance \(r\) from the center \(C\) and an angle \(GCP = \varphi\), erect a perpendicular equal to the value of the expression \[ r^m \sin m \varphi + A r^{m-1} \sin(m-1)\varphi + \text{etc.} + L r\sin \phi, \] which, for brevity, I will always denote by \(T\) in the following. I always consider the distance \(r\) as positive, and for points on the other side of the axis, the angle \(\varphi\) should be considered either as greater than two right angles or as negative (which here is equivalent). The ends of these perpendiculars (which should be taken above the plane for a positive value of \(T\), below for a negative value of \(T\), and on the plane itself when \(T\) vanishes) will be on a continuous curved surface everywhere infinite, which, for brevity, I will call the \textit{first surface} in the following. Similarly, in exactly the same way, another surface, whose height above any point on the plane is \[r^m\cos m\varphi + Ar^{m-1}\cos(m-1)\varphi + \text{etc.}+Lr\cos\varphi+M,\] which I will denote by \(U\) for brevity. This surface will also be continuous and everywhere infinite, and I will distinguish from the former by the term \textit{second surface}. Then it is evident that the whole matter revolves around proving that at least one point exists that lies simultaneously in the plane, on the first surface, and on the second surface.

\subsection*{17.}

It can be easily seen that the first surface lies partly above and partly below the plane; for the distance from the center \(r\) can be taken so large that the remaining terms in \(T\) become negligible compared to the first term \(r^m \sin m \varphi\); this term, however, can be either positive or negative for a properly determined angle \(\varphi\). Therefore, the fixed plane will necessarily intersect the first surface; I will call this intersection of the plane with the first surface the \textit{first line}, which will be determined by the equation \(T = 0\). For the same reason, the plane will intersect the second surface; the intersection will constitute a curve determined by the equation \(U = 0\), which I will call the \textit{second line}. Strictly speaking, each curve will consist of several branches that can be entirely separate, but each will be a continuous line. Indeed, the first line will always be such that it is called a complex, and the axis \(GC\) should be regarded as part of this curve; for any value assigned to \(r\), \(T\) will always be \(= 0\) when \(\varphi\) is either \(= 0\) or \(= 180^o\). However, it is better to consider the complex of all branches passing through all points where \(T = 0\) as one curve (according to the usage generally accepted in higher geometry), and similarly for all branches passing through all points where \(U=0\). It is now evident that the problem has been reduced to proving that at least one point exists in the plane where some branch of the first line intersects some branch of the second line. To achieve this, it will be necessary to closely examine the nature of these lines.

\subsection*{18.}

First of all, I observe that both curves are algebraic, namely, if brought back to orthogonal coordinates, they are of order \(m\). Starting with the abscissas from \(C\), with \(x\) toward \(G\), and ordinates \(y\) toward \(P\), we have \(x = r \cos \varphi\), \(y = r \sin \varphi \), and thus, generally, for any \(n\), \begin{align*} r^n \sin n \varphi = nx^{n-1}y - \tfrac{n . n-1 . n-2}{1 . 2 . 3} x^{n-3} y^3 + \tfrac{n \dots n-4}{1 \dots . 5} x^{n-5}y^5 - \text{etc.}, \\ r^n \cos n \varphi = x^n - \tfrac{n . n-1}{1 . 2} x^{n-2} yy + \tfrac{n . n-1 . n-2 . n-3 }{ 1 . 2 . 3 . 4} x^{n-4}y^4 - \text{etc.} \end{align*} Therefore, both \(T\) and \(U\) will consist of several terms of this kind \(a x^{\alpha} y^{\beta} \), denoting \(\alpha\), \(\beta\) as positive integers whose sum is at most \(= m\). Moreover, it can be easily foreseen that all terms of \(T\) involve the factor \(y\), and therefore, the first line is composed of a line (whose equation is \(y = 0\)) and a curve of order \(m-1\). However, it is not necessary to consider this distinction here.

A matter of greater significance will be the investigation of whether the first and second lines have infinite branches and how many of each. At an infinite distance from the point \(C\), the first line, whose equation is \(\sin m \varphi + \frac{A}{r} \sin (m-1)\varphi + \frac{B}{rr} \sin (m-2) \varphi \text{ etc.} = 0\), will merge with the line whose equation is \(\sin m \varphi = 0\). The latter exhibits \(m\) straight lines intersecting at point \(C\), where the first is the axis \(GCG'\), and the others are inclined at angles \(\frac{1}{m} 180\), \(\frac{2}{m} 180\), \(\frac{3}{m} 180 \) etc. degrees against it. Therefore, the first line has \(2m\) infinite branches, which, when described around the circle with an infinitely large radius, divide the circumference into \(2m\) equal parts. The division occurs in such a way that the circumference is intersected by the first branch at the intersection of the circle and the axis, by the second at a distance of \(\frac{1}{m} 180^o\), by the third at a distance of \(\frac{2}{m} 180^o\), and so on.

Similarly, the second line at an infinite distance from the center will have an asymptote expressed by the equation \(\cos m \phi = 0\). This asymptote is a complex of \(m\) straight lines at point \(C\), intersecting at equal angles, such that the first forms an angle of \(\frac{1}{m}90^o\), the second an angle of \(\frac{3}{m}90^o\), the third an angle of \(\frac{5}{m}90^o\), and so on. Therefore, the second line will also have \(2m\) infinite branches, each occupying the middle position between the two nearest branches of the first line. This arrangement causes the branches to intersect the circumference of a circle described with an infinitely large radius at points that are \(\frac{1}{m}90^o\), \(\frac{3}{m}90^o\), \(\frac{5}{m}90^o\) etc. away from the axis.

However, it is evident that the axis itself always constitutes two infinite branches of the first line, namely the first and \({m+1}^{st}\). This arrangement of the branches is clearly shown in Fig. 2, for the case \(m = 4\), where the branches of the second line are represented with dotted lines to distinguish them from the branches of the first line. The same applies to Fig. 4\footnote{Fig. 4 is constructed assuming \(X = x^4 - 2xx + 3x + 10\), in which case readers less accustomed to general and abstract discussions may find it challenging to visualize the respective positions of both curves concretely. The length of the line \(CG\) is assumed to be 10 (CN= 1.26255.)}. Since these conclusions are of utmost importance, and infinitely large quantities may offend some readers, I will demonstrate them without the support of the infinite in the following article.

\subsection*{19.}

\textsc{Theorem}. \textit{With all the conditions as stated above, a circle can be described from the center \(C\), on whose circumference there are \(2m\) points where \(T=0\) and an equal number of points where \(U=0\), arranged such that each latter point lies between two former points.}

Denote the sum of all coefficients \(A\), \(B\), etc., up to \(K\), \(L\), \(M\) by \(S\), and let \(R\) be taken such that \(R > S\surd{2}\) and \(R > 1\)\footnote{When \(S > \surd{\frac{1}{2}}\), condition one implies condition two; when \(S < \surd{\frac{1}{2}}\), condition two implies condition one.}. Then I say that in a circle described with a radius \(R\), the conditions stated in the theorem necessarily hold. Specifically, for simplicity, designate the point on its circumference that is \( \frac{1}{m}45\) degrees away from its intersection with the left side of the axis, or for which \(\varphi = \frac{1}{m} 45^o\), by (1), and similarly, the point that is \( \frac{3}{m}45^o\) away from this intersection, or for which \(\varphi = \frac{3}{m} 45^o\), by (3); and the point where \(\varphi = \frac{5}{m} 45^o\), by (5), and so on up to \((8m-1)\), which is \( \frac{8m-1}{m} 45\) degrees away from that intersection, if you always progress in the same direction (or \( \frac{1}{m}45^o\) from the opposite side), so that a total of \(4m\) points are on the circumference, spaced at equal intervals.  Then one point will lie between \((8m-1)\) and (1) for which \(T=0\); similarly, there will be singular points between (3) and (5); between (7) and (9); between (11) and (13), and so on, with a total of \(2m\) points. Likewise, each point for which \(U=0\) will lie between (1) and (3); between (5) and (7); between (9) and (11), with the total count also \(=2m\). Finally, apart from these \(4m\) points, there will be no other points in the entire circumference for which either \(T\) or \(U\) is \(= 0\).

\textit{Proof.} I. In the point (1), we have \(m\varphi = 45^o\), and thus \[ T = R^{m-1}(R\surd \tfrac{1}{2} + A \sin (m-1)\varphi + \frac{B}{R} \sin (m-2) \varphi + \text{etc.}+\frac{L}{R^{m-2}}\sin \varphi )\] However, the sum \(\text{etc.}\) involving \(A \sin (m-1) \varphi + \frac{B}{R} \sin (m-2) \phi \) etc. cannot be greater than \(S\). Therefore, it must necessarily be less than \(R\surd{\tfrac{1}{2}}\). It follows that at this point, the value of \(T\) is certainly positive. Hence, \(T\) will have a positive value when \(m\varphi\) lies between \(45^o\) and \(135^o\), i.e., from point (1) to (3), the value of \(T\) will always be positive. By the same reasoning, \(T\) will have a positive value from point (9) to (11) and generally from any point \((8k+1)\) to \(8k+3\), where \(k\) denotes any integer. Similarly, \(T\) will have a negative value everywhere between (5) and (7), between (13) and (15), etc., and generally between \((8k + 5)\) and \((8k+7)\), so it can never be \(= 0\) in these intervals. But since the value is positive at (3) and negative at (5), it must be \(= 0\) somewhere between (3) and (5); also somewhere between (7) and (9); between (11) and (13), etc., up to the interval between \((8m-1)\) and (1) inclusive, so that altogether at \(2m\) points, \(T = 0\). Q.E.D.

II. That no other points with this property exist beyond these \(2m\) points can be understood as follows. Since there are none between (1) and (3), between (5) and (7), etc., it could not be otherwise unless more such points existed, which would happen only if at least two were in some interval between (3) and (5) or between (7) and (9), etc. Then necessarily in the same interval, \(T\) would be either a \textit{maximum} or \textit{minimum}, and thus \(\frac{dT}{d\varphi} = 0\). But \(\frac{dT}{d\varphi} = mR^{m-2}(R\cos m \varphi + \frac{m-1}{m} A \cos(m-1)\varphi + \text{etc.})\), and \(\cos m\varphi\) between (3) and (5) is always negative and \(>\surd{\tfrac{1}{2}}\). Hence, it is easily seen that in this entire interval, \(\frac{dT}{d\varphi}\) is a negative quantity, and similarly, between (7) and (9) everywhere positive; between (11) and (13) negative, etc., so that \(0\) cannot exist in any of these intervals. Therefore, etc. Q.E.S.

III. In a wholly similar manner, it is demonstrated that \(U\) has a negative value everywhere between (3) and (5), between (11) and (13), etc., and generally between \((8k+3)\) and \((8k+5)\); positive, however, between (7) and (9), between (15) and (17), etc., and generally between \( (8k+7)\) and \((8k+9)\). Hence, it immediately follows that \(U=0\) must occur somewhere between (1) and (3), between (5) and (7), etc., i.e., in \(2m\) points. However, in none of these intervals can \(\frac{dU}{d\varphi}=0\) occur (which is easily proved similarly as above): therefore, more than those \(2m\) points on the circumference of the circle will not be given, where \(U=0\). Q.E.T. and Q.

Moreover, the part of this theorem according to which more than \(2m\) points do not exist where \(T=0\), nor more than \(2m\) where \(U = 0\), can also be demonstrated from the fact that the equations \(T = 0\), \(U=0\) represent curves of \(m^{th}\) order, such as, according to higher geometry, cannot be cut in more than \(2m\) points, a circle being a curve of the second order.

\subsection*{20.}

If another circle with a radius greater than \(R\) is described from the same center, then it will be divided in the same way: between points (3) and (5), there will be one point where \(T=0\), likewise between (7) and (9), etc. It will be easily observed that the less the radius of this circle differs from the radius \(R\), the closer such points between (3) and (5) should be on the circumferences of both circles. The same will occur if a circle with a radius somewhat smaller than \(R\) but greater than \(S\surd{2}\) and \(1\) is described. From this, it is easily understood that the circumference of the circle described with a radius \(R\) is actually \textit{cut} at the point between (3) and (5) where \(T=0\) by some branch of the first line; the same holds for the other points where \(T=0\). Similarly, it is evident that the circumference of this circle is cut at all \(2m\) points where \(U=0\) by some branch of the second line. These conclusions can also be expressed in the following way: When a circle of the appropriate size is described from the center \(C\), \(2m\) branches of the first line and \(2m\) branches of the second line will enter this, in such a way that the two nearest branches of the first line are separated by some branch of the second line. See Fig. 2, where the circle is now of finite size, and the numbers assigned to each branch are not to be confused with the numbers by which I designated specific limits in the previous article and in this for the sake of brevity.

\subsection*{21.}

Now, from this relative arrangement of the branches entering the circle, the intersection of some branch of the first line with a branch of the second line within the circle can be deduced in various ways. I am almost ignorant of which method to choose among the rest. The following seems very clear: Let's designate (Fig. 2) a point on the circumference of the circle, where it is cut by a branch from the left side of the axis (which itself is one of the \(2m\) branches of the first line) as \(0\); the nearest point where a branch of the second line enters, as \(1\); the next point to this, where the second branch of the first line enters, as \(2\), and so on up to \(4m-1\), so that in any point marked with an even number, a branch of the second line enters the circle, contrary to a branch of the first line expressed in all points by an odd number. It is well known from higher geometry that an algebraic curve (or each part of any algebraic curve if it happens to be composed of several) may either return into itself or extend infinitely on both sides, so if any branch of an algebraic curve enters a finite space, it must necessarily come out again somewhere from this space\footnote{It seems to have been demonstrated quite well that an algebraic curve cannot suddenly break off anywhere (as happens, for example, in a transcendental curve whose equation is \( y = \frac{1}{\log x} \)), nor lose itself, as it were, after infinite spirals at some point (like the logarithmic spiral), and as far as I know, no one has cast doubt on this matter. However, if someone demands a demonstration that is not subject to any doubts, I will undertake it on another occasion. In the present case, it is evident that if a branch, for example, 2, did not come out from the circle anywhere (Fig. 3), you could enter the circle between \(0\) and \(2\), then move around the whole branch (which should get lost in the space of the circle), and finally be able to exit between \(2\) and \(4\) again, so that you never intersect the first line on the entire path. This is absurd because at the point where you entered the circle, you had the first surface above you, and in the exit, below; therefore, you must have necessarily intersected the first surface itself somewhere, i.e., at a point on the first line. However, from this reasoning based on the principles of the geometry of position, which are no less valid than the principles of the geometry of magnitudes, it follows only that if you enter a branch of the first line in the circle, you can exit somewhere else from the circle, always remaining on the first line, and not that your path is a continuous line in the sense in which it is understood in higher geometry. But here it suffices that the path is a continuous line in the common sense, i.e., not interrupted anywhere but cohering everywhere.}. Hence, it is easily concluded that any point marked with an even number (or, for the sake of brevity, any even point) should be connected by a branch of the first line with another even point within the circle, and similarly, any point marked with an odd number should be connected with another similar point by a branch of the second line. Although the connection of these two points according to the nature of the function \(X\) can be very different, so that it cannot be determined in general, it can be easily demonstrated that \textit{in any case, an intersection of the first line with the second line always occurs.}

\subsection*{22.}

The demonstration of this necessity seems most conveniently representable by reductio ad absurdum. Namely, let's assume that the connection of any two even points and any two odd points can be arranged in such a way that no intersection of a branch of the first line with a branch of the second line arises from it. Since the axis is a part of the first line, clearly point \(0\) must be connected with point \(2m\). Therefore, point \(1\) cannot be connected with any point beyond the axis, i.e., with no point expressed by a number greater than \(2m\), otherwise the connecting line would necessarily cut the axis. So, if \(1\) is assumed to be connected with point \(n\), then \(n < 2m\). By similar reasoning, if \(2\) is connected with \(n'\), then \(n' < n\), because otherwise, the branch \(2...n'\) would necessarily cut the branch \(1...n\). For the same reason, point \(3\) will be connected with some point between \(4\) and \(n'\), and it is clear that if \(3\), \(4\), \(5\), etc., are assumed to be connected with \(n''\), \(n'''\), \(n''''\), etc., \(n'''\) lies between \(5\) and \(n''\), \(n''''\) between \(6\) and \(n'''\), etc. Hence, it is evident that, finally reaching some point \(h\) connected with point \(h+2\), the branch entering the circle at point \(h+1\) will necessarily cut the branch connecting points \(h\) and \(h+2\). However, since one of these two branches will belong to the first line and the other to the second, it is now clear that the assumption is contradictory, and therefore, an intersection of the first line with the second line must necessarily occur somewhere.

If this is combined with the preceding discussions, it will be concluded from all the explanations that the theorem, \textit{a rational algebraic function of one indeterminate can be resolved into factors of the first or second degree with real coefficients}, has been rigorously demonstrated.

\subsection*{23.}

Moreover, it can be easily deduced from the same principles, that not only one but at least \(m\) intersections of the first line with the second line are given, although it is also possible for the first line to be cut by several branches of the second line at the same point, in which case the function \(X\) will have multiple equal factors. However, since it suffices here to have demonstrated the necessity of one intersection, I do not dwell further on this matter for the sake of brevity. For the same reason, I do not pursue other properties of these lines here in more detail, such as the intersection always occurring at right angles, or if multiple branches of each curve coincide at the same point, the first line having as many branches as the second line, and these being alternately placed, intersecting at equal angles, etc.

Finally, I observe that it is not impossible for the preceding demonstration, which I built on geometric principles here, to be presented in a purely analytical form. However, I believed that the representation I explained here would be less abstract, and the essence of the proof could be put more clearly before the eyes than could be expected from an analytical demonstration.

As a bonus, I will suggest another method for proving our theorem, which, at first glance, will seem not only very different from the preceding demonstration but also from all the other demonstrations explained above, and yet it is fundamentally the same as the \textsc{d'Alembert}ian method. I leave it to those familiar with the subject to compare it with the previous one and explore the parallelism between the two. It is attached solely for their benefit.

\subsection*{24.}

Above the plane of Figure 4, relative to the axis \(CG\) and the fixed point \(C\), I assume that the first and second surfaces are described in the same way as above. Take any point located on any branch of the first line, where \(T = 0\) (for example, any point \(M\) lying on the axis), and unless \(U = 0\) at this point, proceed from this point in the first line towards the direction where the absolute magnitude of \(U\) decreases. If, by chance, the absolute value of \(U\) decreases in both directions at the point \(M\), it is arbitrary where you proceed; but I will immediately explain what to do if \(U\) increases in both directions. It is clear that as long as you always progress in the first line, you will necessarily reach a point where \(U = 0\) or one where the value of \(U\) becomes a minimum, for example, the point \(N\). In the former case, the sought point is found; in the latter, it can be demonstrated that in this point, multiple branches of the first line intersect (indeed, an equal number of branches), and their semiaxes are so arranged that if you deviate towards any of them (either here or there), the value of \(U\) will continue to decrease. (For the sake of brevity, I must suppress the demonstration of this theorem, which, although not more difficult, is more extensive.) In this branch, you can then progress again until \(U\) becomes \(= 0\) (as happens in Fig. 4 at \(P\)) or again a minimum. Then, deviating again, you will necessarily reach a point where \(U = 0\).

Against this demonstration, a doubt could be raised about whether it is possible that no matter how far you progress, and even though the value of \(U\) always decreases, these decrements continuously become slower, and nevertheless, that value never reaches a certain limit. This objection would correspond to the fourth in Article 6. But it would not be difficult to assign a limit, such that once you surpass it, the value of \(U\) must necessarily not only change more rapidly but also \textit{not decrease} any longer, so that before reaching this limit, the value \(0\) must have necessarily been reached. However, I reserve the opportunity to elaborate more extensively on this and other points that I could only touch upon in this demonstration on another occasion.

\begin{center}

 \rule{3in}{0.5pt}

\begin{scriptsize}\textit{We discovered the principles on which this demonstration is based in October 1797.}\end{scriptsize}

 \rule{2in}{0.5pt}
\end{center}
\pagebreak
\begin{center}
\includegraphics[width=\linewidth]{Section1Figures.png}
\end{center}

 \end{document}









\end{document}

\documentclass{book}
\usepackage{standalone}
\usepackage[dvips,text={6.5truein,9truein},left=1truein,top=1truein]{geometry}
\usepackage{amsmath, amsthm}
\usepackage{titlesec}

% Uncomment to use syncing
%\usepackage{pdfsync}


% Paragraphs
\parindent=0pt
\parskip=5 pt plus 2 pt minus 1pt

\titleformat{\section}
 {\normalfont\large\bfseries\centering}{\thesection.}{1em}{}

\titleformat{\subsection}
 {\normalfont\normalsize\centering}{\thesection.}{1em}{}

\theoremstyle{plain}
\newtheorem*{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{problem}{Problem}


\theoremstyle{remark}
\newtheorem*{example}{Example}
\newtheorem*{examples}{Examples}

\begin{document}

\title{ CARL FRIEDRICH GAUSS \\ WERKE \\ BAND III.}  
\date{}
\maketitle

\section*{DEMONSTRATION OF A NEW THEOREM: EVERY RATIONAL ALGEBRAIC FUNCTION OF A SINGLE VARIABLE CAN BE RESOLVED INTO REAL FACTORS OF FIRST OR SECOND DEGREE.}

\subsection*{1.}

Any given algebraic equation can be reduced to the form \[ x^m + Ax^{m-1} + Bx^{m-2} + \text{etc.} + M = 0 \] where \(m\) is a positive integer. If we denote the first part of this equation by \(X\), and suppose that the equation \(X = 0\) is satisfied by several unequal values of \(x\), say by setting \(x=\alpha, x =\beta, x = \gamma\) etc., the function \(X\) will be divisible by the product of the factors \(x-\alpha, x-\beta, x-\gamma\) etc. Conversely, if the product of several simple factors \(x-\alpha, x-\beta, x-\gamma\) etc. measures the function \(X\), it satisfies the equation \(X= 0\) by making \(x\) equal to any of the quantities \(\alpha, \beta, \gamma\) etc. Finally, if \(X\) is equal to the product of \(m\) such simple factors (whether all distinct or some of them identical), no other simple factors besides these can measure the function \(X\). Therefore, an equation of degree \(m\) cannot have more roots than \(m\). However, it is evident that an equation of degree \(m\) can have \textit{fewer} roots, even if \(X\) is resolvable into \(m\) simple factors. If some of these factors are identical, the number of different ways in which the equation can be satisfied will necessarily be less than \(m\). Nevertheless, for the sake of elegance, geometers preferred to say that the equation, in this case as well, has \(m\) roots, some of which are equal to each other: a liberty they could certainly allow themselves.

\subsection*{2.}

What has been explained so far is sufficiently demonstrated in algebraic works and does not violate geometric rigor anywhere. However, analysts seem to have adopted the theorem too hastily and without a solid prior demonstration, a theorem upon which almost the entire theory of equations is built: \textit{Although a function such as \(X\) can always be resolved into \(m\) simple factors, whether this entirely agrees with that, and although an equation of degree \(m\) indeed has \(m\) roots}. Since situations arise frequently in quadratic equations that contradict this theorem, algebraists, in order to subsume these cases, were compelled to introduce a certain imaginary quantity whose square is \(-1\). They then acknowledged that if quantities of the form \(a+b\surd{-1}\) are treated as real, the theorem is not only valid for quadratic equations but also for cubic and biquadratic ones. However, it does not follow from this that, by admitting quantities of the form \(a+b\surd{-1}\), any equation of the fifth degree or higher can be satisfied, or as is often expressed (though I prefer a less slippery phrase), the roots of such an equation can be reduced to the form \(a+b\surd{-1}\). This theorem does not differ from what is stated in the title of this paper when you consider the matter itself, and my aim in this dissertation is to present a new rigorous proof of it.

Moreover, since the time analysts discovered that there are infinitely many equations that have no roots at all unless quantities of the form \(a+b\surd{-1}\) are admitted—these fictitious quantities, called imaginary, to distinguish them from real ones, have been considered and introduced into the entire analysis; by what right? I do not argue this point here. — I will complete my demonstration without any recourse to imaginary quantities, although, with the same freedom that all recent analysts have used, I could have availed myself of them.

\subsection*{3.}

Although what is commonly presented in many elementary books as a demonstration of our theorem is so trivial and so far removed from geometric rigor that it is hardly worth mentioning, I will briefly touch upon it to avoid any appearance of omission. To demonstrate that the equation \[x^m + Ax^{m-1} + Bx^{m-2} + \text{etc.} + M = 0\] or \(X = 0\) indeed has \(m\) roots, they attempt to prove that \(X\) can be resolved into \(m\) simple factors. For this purpose, they assume \(m\) simple factors \(x-\alpha, x-\beta, x-\gamma\) etc., where \(\alpha, \beta, \gamma\) etc. are still unknown, and they set the product of these factors equal to the function \(X\). Then, by comparing coefficients, they derive \(m\) equations, claiming that the unknowns \(\alpha, \beta, \gamma\) etc. can be determined, the number of which is also \(m\). Namely, \(m-1\) of the unknowns can be eliminated, leading to an equation containing only the single unknown \(\alpha\), for instance. As for the remaining issues that could be criticized in such reasoning, I will only ask: How can we be certain that the final equation actually has any roots? Why couldn't it be the case that neither this final equation nor any quantity within the entire scope of real and imaginary numbers satisfies the proposed equation? — However, experts will easily perceive that this final equation must necessarily be \textit{entirely identical} to the proposed one if the calculus is conducted correctly; namely, eliminating the unknowns \(\beta, \gamma\) etc., it should result in the equation \[ \alpha^m + A\alpha^{m-1} + B\alpha^{m-2} + \text{etc.} + M = 0. \] There is no need to elaborate further on this reasoning.

Some authors, who seem to have perceived the weakness of this method, assume as an \textit{axiom} that even if an equation indeed has roots, they are either possible or impossible. What they mean by possible and impossible quantities does not seem to be explained distinctly enough. If possible quantities are supposed to denote the same as real and impossible ones the same as imaginary, this axiom cannot be admitted without a demonstration but necessarily requires one. However, it seems that expressions should not be understood in this sense, but rather the intention of the axiom appears to be: `Although we are not yet certain that \(m\) real or imaginary quantities necessarily exist satisfying a given equation of degree \(m\), we will assume it for a while; for if it should happen that so many real and imaginary quantities cannot be found, there will certainly be an escape, allowing us to say that the remaining ones are impossible.' If someone prefers to use this phrase rather than simply stating that the equation, in this case, will not have so many roots, I have no objection; but if, in using these impossible roots at that time as if they were something true, he says, for example, that the sum of all the roots of the equation \(x^m + A x^{m-1} + \text{etc.} = 0\) is \(= -A\), even if some of them are impossible (which expression clearly means, \textit{even if some are missing}), I cannot prove this. Because impossible roots, in this sense, are still roots, and then the axiom cannot be admitted without a demonstration. Nor would it be unreasonable to doubt whether equations can exist that do not even have impossible roots.\footnote{By imaginary quantity, I always understand a quantity in the form \(a+b\surd{-1}\), as long as \(b\) is not equal to \(0\). In this sense, this expression has always been accepted by all geometers of the first rank, and I consider those who wanted to call a quantity \(a+b\surd{-1}\) imaginary only in the case where \(a = 0\), and impossible when \(a\) is not equal to \(0\), neither necessary nor useful. — If imaginary quantities are to be retained in analysis at all (which seems wiser than abolishing them altogether, provided they are solidly established): necessarily, they should be regarded as equally possible as real ones; therefore, I would prefer to include real and imaginary quantities under the common designation of \textit{possible quantities}, and, conversely, I would call a quantity \textit{impossible} if it must satisfy conditions that cannot be satisfied even by imaginary ones, yet in such a way that this expression means the same as saying that such a quantity does not exist within the entire range of magnitudes. However, I would not agree to forming a separate class of quantities for this. If someone says that an equilateral right-angled triangle is impossible, no one will deny it. But if he wants to contemplate such a triangle as a new kind of triangles and apply other properties of triangles to it, who would not laugh? This would be playing with words, or rather abusing them. — Although even the greatest mathematicians have often applied truths, which manifestly presuppose the possibility of quantities to which they refer, to such quantities as well, the possibility of which was still doubtful; and although I do not deny that such licenses usually pertain only to the form and, as it were, a veil of reasoning, which the acuity of true geometers can soon penetrate: yet it seems more advisable, and more worthy of the sublimity for which this science, celebrated as the most perfect example of clarity and certainty, is held, either to entirely proscribe such liberties, or at least to use them sparingly and only where less practiced individuals may have difficulty perceiving the matter even without their aid, although it might be accomplished less briefly but just as rigorously. — However, I will not deny that what I said here against the abuse of impossibilities can, in a certain respect, also be applied against imaginaries: but I reserve the vindication of these, as well as a more ample exposition of this whole matter, for another occasion.}

\subsection*{4.}

Before reviewing the proofs of our theorem by other geometers and pointing out what, in each, seems to me worthy of criticism, I will explain: I observe that it is sufficient to show only that for any equation of any degree \[x^m + Ax^{m-1} + B x^{m-2} + \text{etc.} + M = 0\]
or \(X = 0\) (where the coefficients \(A, B\) etc. are assumed to be real), it is possible to satisfy it in at least one way by the value of \(x\) expressed as \(a+b\surd{-1}\). For it is evident that \(X\) will then be divisible by the real quadratic factor \(xx-2ax+aa+bb\) if \(b\) is not equal to \(0\), and by the real simple factor \(x-a\) if \(b = 0\). In both cases, it will be real and of lower degree than \(X\); and since, for the same reason, it must have a real factor of the first or second degree, it is clear that, by continuing this operation, the function \(X\) will eventually be resolved into simple or double real factors, or, if you prefer to use two simple imaginary factors for each real double factor, into \(m\) simple factors.

\subsection*{5.}

The first proof of the theorem is owed to the distinguished geometer \textsc{d'Alembert}, \textit{Recherches sur le calcul integral, Histoire de l'Acad. de Berlin, Ann\'ee} 1746, p. 182 sqq. It is also found in \textsc{Bougainville}, \textit{Trait\'e du calcul integral, \`a Paris} 1754, p. 47 sqq. The main points of this method are as follows.

First, he shows that if any function \(X\) of a variable quantity \(x\) becomes \(= 0\) either for \(x = 0\) or for \(x = \infty\), and can acquire a real positive infinitely small value by assigning to \(x\) a real value, then this function can also obtain a real negative infinitely small value by the value of \(x\) either real or in the form of the imaginary \(p+q\surd{-1}\). Namely, denoting \(\Omega\) as the infinitely small value of \(X\), and \(\omega\) as the corresponding value of \(x\), he asserts that \(\omega\) can be expressed by a highly convergent series \(a\Omega^{\alpha} + b \Omega^{\beta} + c \Omega^{\gamma} \) etc., where the exponents \(\alpha, \beta, \gamma\) etc. are continuously increasing rational quantities that become positive values at least at a certain distance from the starting point and make the terms in which they appear infinitely small. Now, if among all these exponents there is none that is a fraction with an even denominator, all the terms of the series become real for both positive and negative values of \(\Omega\); but if some fractions with an even denominator are found among these exponents, it is established that for the negative value of \(\Omega\), the corresponding terms are contained in the form \(p+q\surd{-1}\). However, due to the infinite convergence of the series in the former case, it is sufficient to retain only the first (i.e., the largest) term, and in the latter case, it is not necessary to go beyond the term that first introduces the imaginary part.

Through similar reasoning, it can be shown that if \(X\) can obtain a negative infinitely small value from the real value of \(x\), that function can acquire a real positive infinitely small value from the real value of \(x\) or from the imaginary one in the form \(p+q\surd{-1}\).

Hence, secondly, he concludes that a certain finite real value of \(X\) is given, negative in the former case, positive in the latter, which can be produced from the imaginary value of \(X\) in the form \(p+q\surd{-1}\).

From this, it follows that if \(X\) is such a function of \(x\) that it obtains a real value \(V\) from the real value \(v\) of \(x\), and also obtains a real value infinitely small or greater or smaller than the real value \(v\) of \(x\), it can also receive the same real value infinitely small and hence finite, or smaller or greater than \(V\) (respectively), by assigning to \(x\) a value in the form \(p+q\surd{-1}\). This is easily derived from the preceding if we conceive \(X\) to be replaced by \(V + Y\) and \(x\) by \(v+y\).

Finally, he asserts that if \(X\) is assumed to be able to traverse any interval between two real values \(R, S\) (i.e., becoming equal to \(R\) itself, then to \(S\) itself, and then to all intermediate real values), by assigning to \(x\) values always in the form \(p+q\surd{-1}\); then the function \(X\) can be increased or decreased by any finite real quantity (depending on whether \(S>R\) or \(S<R\)), while \(x\) remains always in the form \(p+q\surd{-1}\). For if a real quantity \(U\) were given (between which \(R\) and \(S\) are supposed to lie), which \(X\) could not become equal to by such a value of \(x\), necessarily the \textit{maximum} value of \(X\) would be given (namely when \(S>R\); and the minimum, when \(S<R\)), say \(T\), which he would obtain from the value of \(x\), \(p+q\surd{-1}\), in such a way that no value of \(x\) in a similar form could be assigned that would bring the function \(X\) closer to \(U\) with a minimum excess. Now, if in the equation between \(X\) and \(x\), \(p+q\surd{-1}\) is substituted for \(x\) everywhere, and then the real part and the part involving the factor \(\surd{-1}\) are equated, two equations result from this (in which \(p\), \(q\), and \(X\) occur mixed with constants) from which two other equations can be obtained by elimination. In one of them, \(p, X\), and constants are found, and in the other, \(p\) is free containing only \(q\), \(X\), and constants. Therefore, since \(X\) traverses all values from \(R\) to \(T\) by real values of \(p, q\), according to the preceding, it can still approach the value \(U\) by assigning values such as \(\alpha + \gamma \surd{-1}\) and \(\beta + \delta\surd{-1}\) to \(p, q\), respectively. Hence, \(x = \alpha - \delta + (\gamma + \beta)\surd{-1}\) can still be in the form \(p+q\surd{-1}\) against the hypothesis.

Now, if \(X\) is supposed to denote a function such as \(x^m + A x^{m-1} + B x^{m-2} + \text{etc.} + M\), it is easily understood that real values can be assigned to \(x\) such that \(X\) traverses any interval between two real values. Therefore, \(x\) can also obtain a certain value in the form \(p+q\surd{-1}\), from which \(X\) becomes \(=0\). Q.E.D.\footnote{It is worth noting that the illustrious \textsc{d'Alembert}, in his exposition of this demonstration, employed geometric considerations and regarded \(X\) as the abscissa and \(x\) as the ordinate of a curve (in accordance with the custom of most geometers of the first part of this century, among whom the notion of functions was less common). However, since all his reasoning, if considered solely in its essence, relies on purely analytic principles, and imaginary curves and imaginary ordinate expressions may appear more cumbersome and could potentially confuse the modern reader, I chose to use a purely analytic form of representation here. I added this note to ensure that anyone comparing the \textsc{d'Alembertian} demonstration itself with this concise exposition does not suspect any essential changes.}

\subsection*{6.}

The objections that seem to be raised against the \textsc{d'Alembert}ian demonstration can be summarized as follows:

\begin{enumerate}
    \item[1.] \textsc{d'Alembert} raises no doubt about the \textit{existence} of values for \(x\) that correspond to given values of \(X\), but he assumes this and only investigates the \textit{form} of these values. While this objection is inherently significant, it pertains only to the form of expression, which can be easily corrected to completely eliminate it.

    \item[2.] The assertion that \(\omega\) can always be expressed by such a series as stated is certainly false if \(X\) is meant to denote any transcendent function (as \textsc{d'Alembert} implies in several places). This is evident, for example, if we take \(X = e^{\frac{1}{x}}\) or \(X = \frac{1}{\log X}\). However, if we restrict the demonstration to the case where \(X\) is an algebraic function of \(x\) (which suffices for the present purpose), the proposition is indeed true. \textsc{d'Alembert} did not provide any evidence to support his assumption; the renowned \textsc{Bougainville} assumes that \(X\) is an algebraic function of \(x\) and recommends employing the \textsc{Newtonian} parallelogram series for finding it.

    \item[3.] \textsc{d'Alembert} uses infinitely small quantities more freely than can be justified with geometric rigor, at least in our era where they rightfully face skepticism from analysts. Additionally, he does not explain the transition from the value of an infinitely small quantity \(\Omega\) to a finite value sufficiently lucidly. The proposition that \(\Omega\) can attain some finite value seems to be concluded not so much from the possibility of an infinitely small value of \(\Omega\) but rather from the fact that, with \(\Omega\) denoting a very small value, the closer one gets to the true value of \(\omega\) by considering more terms in the series, or by taking the sum of more terms as \(\omega\). Furthermore, this entire argument appears too vague to draw any rigorous conclusions. It is worth noting that there are series, even with arbitrarily small values assigned to the quantity they depend on, that still diverge as long as they are continued, reaching larger terms for any given quantity\footnote{On this occasion, I briefly note that many series appear highly convergent at first glance, especially most of those used by \textsc{Euler} in the later part of \textit{Institutiones Calculi Differentialis}, Chapter VI, for approximating the sum of other series (since the remaining series on pages 475—478 can indeed converge). As far as I know, this has not been observed by anyone to date. Therefore, it is highly desirable for a clear and rigorous demonstration to be presented, explaining why such series, which initially converge very quickly, then gradually converge more slowly and finally diverge more and more, can still provide the closest true sum as long as not too many terms are taken and up to what point such a sum can be safely considered exact.}. This happens when the coefficients of the series form a hypergeometric progression. Therefore, it should have been necessarily demonstrated that such a hypergeometric series cannot arise in the present case.

    In any case, it seems to me that \textsc{d'Alembert} did not correctly resort to infinite series here and that these are not suitable for establishing this fundamental theorem of the theory of equations.

    \item[4.] From the assumption that \(X\) can attain the value \(S\) but not the value \(U\), it does not necessarily follow that between \(S\) and \(U\) there must be a value \(T\) that \(X\) can reach but not exceed. Another case remains: namely, it could happen that there is a limit between \(S\) and \(U\) which \(X\) can approach as closely as desired but never actually reach. From the arguments presented by \textsc{d'Alembert}, it only follows that \(X\) can always exceed any value it has reached by a finite quantity, for example, when it has become \(= S\), it can still increase by some finite quantity \(\Omega\), and with this, a new increment \(\Omega'\) can be added, then another increase \(\Omega''\), etc., so that no matter how many increments have already been added, none should be considered the last, and a new one can always be added. Although the multitude of possible increments is not bounded by any limits, it could still happen that if the increments \(\Omega, \Omega', \Omega''\) etc. continuously decrease, the sum \(S +\Omega + \Omega' + \Omega''\) etc. may never reach a certain limit, no matter how many terms are considered.

    While this case cannot occur when \(X\) denotes an entire algebraic function of \(x\), without a demonstration that it cannot happen, the method must be considered incomplete. However, when \(X\) is a transcendental function or even an algebraic fraction, this case can indeed occur, for example, whenever a value of \(X\) corresponds to an infinitely large value of \(x\). In such cases, the \textsc{d'Alembertian} method seems to be not without many ambiguities and, in some cases, may not be reducible to indubitable principles.

\end{enumerate}

For these reasons, I cannot consider the \textsc{d'Alembertian} demonstration as satisfactory. Nevertheless, despite this, I do not believe that the true essence of the demonstration, proving against all objections, is necessarily undermined. I think that, based on the same foundation (though by far different reasoning and at least with greater circumspection), not only a rigorous demonstration of our theorem can be constructed but also everything can be derived from it that might be desired concerning the theory of \textit{transcendental} equations. I will discuss this matter more extensively on another occasion; see, in the meantime, Article 24 below.

\subsection*{7.}

After \textsc{d'Alembert}, illustrious \textsc{Euler} published his investigations on the same subject, \textit{Recherches sur les racines imaginaires des equations, Hist. de l'Acad. de Berlin A.} 1749, p. 223 sqq. He presented a dual method, and the summary of the first is as follows:

First, \textsc{Euler} aims to demonstrate that, for any dignity \(m\) of the number \(2\), the function \(x^{2m} + Bx^{2m-2} + C x^{2m-3} + \text{etc.} + M = X\) (where the coefficient of the second term is \(= 0\)) can always be resolved into two real factors, in which \(X\) ascends to \(m\). To this end, he assumes two factors: \[x^m - u x^{m-1} + \alpha x^{m-2} + \beta x^{m-3} + \text{etc., and } x^m + u x^{m-1}+\lambda x^{m-2} + \mu x^{m-3} + \text{etc.} \] where the coefficients \(u, \alpha, \beta\), etc., \(\lambda, \mu\) etc. are still unknown, and their product is set equal to the function \(X\). Then, the comparison of coefficients provides \(2m-1\) equations, and it only needs to be clearly demonstrated that the unknowns \(u, \alpha, \beta\), etc., \(\lambda, \mu\) (whose number is also \(2m -1\)) can be assigned such real values that satisfy these equations. Now, \textsc{Euler} asserts that if \(u\) is considered as known first, so that the number of unknowns is one less than the number of equations, by properly combining these according to algebraic methods, all \(\alpha, \beta\) etc., \(\lambda, \mu\) etc. can be rationally determined without any extraction of roots, depending on \(u\) and the coefficients \(B, C\) etc., and therefore real values can be obtained once \(u\) becomes real. Furthermore, all \(\alpha, \beta\) etc., \(\lambda, \mu\) etc. can be eliminated so that an equation \(U = 0\) is produced, where \(U\) is an entire function of \(u\) and known coefficients alone. It would be immensely laborious to evolve this equation through the ordinary method of elimination when the given equation \(X = 0\) is of a somewhat high degree; and for an indeterminate degree, it would be completely impossible (as stated by \textsc{Euler} himself on p. 239). However, it suffices here to know one property of that equation, namely, that the last term in \(U\) (which does not involve the unknown \(u\)) must be negative. It can then be inferred that the equation must have at least one real root, i.e., \(u\) and therefore \(\alpha, \beta\) etc., \(\lambda, \mu\) etc. can be determined in at least one real way. This property can be confirmed by the following reflections: When \(x^m - u x^{m-1}+\alpha x^{m-2} +\) etc. is supposed to be a factor of the function \(X\), \(u\) will necessarily be the sum of \(m\) roots of the equation \(X = 0\), and therefore, it must have as many values as there are different ways to select \(m\) roots from \(2m\) roots, i.e., according to the principles of the calculus of combinations \(\frac{2m.(2m-1).(2m-2).\dots.(m+1)}{1.2.3 \dots m}\). This number will always be a product of an odd number and an even number (a not difficult demonstration is omitted here): if it is denoted as \(= 2k\), its half \(k\) will be odd. Therefore, the equation \(U= 0\) will be of degree \(2k\). Now, since the second term is missing in the equation \(X= 0\): the sum of all \(2m\) roots will be \(0\); from this, it is evident that if the sum of any \(m\) roots is \(+p\), the sum of the remaining roots will be \(-p\), i.e., if \(+p\) is among the values of \(u\), even \(-p\) will be among the same values. Hence, \textsc{Euler} concludes that \(U\) is a product of \(k\) double factors such as \(uu - pp, uu - qq, uu - rr\) etc., denoting \(+p, -p, +q, -q\) etc., all \(2k\) roots of the equation \(U=0\), where, due to the odd number of factor pairs, the last term in \(U\) will be the square of the product \(pqr\) etc., with a negative sign. However, the product \(pqr\) etc. can always be rationally determined from the coefficients \(B, C\) etc., and therefore, it must necessarily be a real quantity. Thus, the square of this quantity will certainly be negative. Q.E.D.

Since these two real factors of \(X\) are of degree \(m\), and \(m\) is the power of the number \(2\), by the same reasoning, each of them can be resolved again into two real factors of \(\tfrac{1}{2}m\) dimensions. However, since repeated halving of the number \(m\) necessarily leads to a binary, it is evident that by continuing this operation, the function \(X\) can ultimately be resolved into real factors of the second degree.

Now, if a function is proposed in which the second term is not absent, for example, \(x^{2m} + A x^{2m-1} + B x^{2m-2} + \text{etc.} + M\), still denoting \(2m\) as the binary power, this will pass into a similar function lacking the second term through the substitution \(x = y - \frac{A}{2m}\). Hence, it is easily concluded that even this function can be resolved into real factors of the second degree.

Finally, for a given function of degree \(n\), denoting \(n\) as a number that is not a binary power, let the binary power immediately greater than \(n\) be \(= 2m\). Multiply the given function by \(2m - n\) simple real factors of any kind. From the resolvability of the product into real factors of the second degree, it is easily derived that the given function must also be resolvable into real factors of the second or first degree.


\end{document}


\section*{PROOF OF THE NEW THEOREM THAT EVERY INTEGRAL ALGEBRAIC FUNCTION OF ONE VARIABLE CAN BE RESOLVED INTO REAL FACTORS OF THE FIRST OR SECOND DEGREE.}

\subsection*{ 1. }

Any given algebraic equation can be reduced to the form \[ x^m + Ax^{m-1} + Bx^{m-2} + \text{etc.} + M = 0 \] where \(m\) is a positive integer. If we denote the first part of this equation by \(X\), and we assume that the equation \(X = 0\) is satisfied by several unequal values of \(x\), for example by setting \(x=\alpha, x =\beta, x = \gamma\) etc., the function \(X\) will be divisible by the product of the factors \(x-\alpha, x-\beta, x-\gamma\) etc. Conversely, if the function \(X\) is divisible by the product of several simple factors \(x-\alpha, x-\beta, x-\gamma\) etc., then the equation \(X= 0\) will be satisfied by equating \(x\) to any of the quantities \(\alpha, \beta, \gamma\) etc. Finally, if \(X\) is equal to the product of \(m\) such simple factors (whether they are all different or some of them identical), then other simple factors besides these cannot divide the function \(X\). Therefore, an equation of the \(m^{th}\) degree cannot have more than \(m\) roots; but at the same time it is clear that an equation of the \(m^{th}\) degree can have \textit{fewer} roots, even if \(X\) is resolvable into \(m\) simple factors.  For if among these factors some are identical, the number of different ways to satisfy the equation will necessarily be smaller than \(m\). However, for reasons of congruity, the geometers preferred to say that in this case the equation also had \(m\) roots, and only that some of them were equal to each other: which they could certainly allow themselves.

\subsection*{2.}

What has been narrated up to now is sufficiently demonstrated in algebraic books and does not offend geometric rigor anywhere. But the analysts seem to have adopted the theorem, on which almost the whole teaching of equations is based, too prematurely and without prior solid demonstration: \textit{Any function such as \(X\) can always be resolved into \(m\) simple factors}, or the fact that it is exactly the same, \textit{Any equation of degree  \(m\) has \(m\) roots}.  Now, already in equations of the second degree we very often come to cases which contradict this theorem.  The algebraists, in order to reconcile this, were forced to invent a certain imaginary quantity whose square is \(-1\), and then they recognized that if the quantities of the form \(a+b\surd{-1}\) are likewise allowed to be real, the theorem would be true not only for equations of the second degree, but also for cubic and biquadratic ones. From here, however, it is not permissible to infer that any equation of the fifth or higher degree can be satisfied by quantities of the form \(a+b\surd{-1}\), or as it is usually expressed (although I would prefer a less slippery phrase), that the roots of such an equation can be reduced to the form \(a +b\surd{-1}\). This theorem is no different from that which is stated in the title of this manuscript, if you look into the matter, and to give a new rigorous demonstration of this theorem constitutes the purpose of the present dissertation.

Moreover, from the time when analysts discovered that there were infinitely many equations which would have no root at all unless quantities of the form \(a+b\surd{-1}\) were admitted, such fictitious quantities were considered as a special class of quantities, which they called imaginary, in order to distinguish them from the real ones, and were introduced into the whole analysis.  By what right? I will not attempt to dispute this.  I will complete my demonstration without the aid of any imaginary quantities, although I could have used them as freely as all modern analysts have.

\subsection*{3.}

That which is given as a demonstration of our theorem in most elementary books is so flimsy, and so far removed from geometrical rigour, that it is scarcely worthy of mention.  Nevertheless, lest anything should appear to be lacking, I will touch upon it in a few words. `To show that the equation 
\[x^m + Ax^{m-1} + Bx^{m-2} + \text{etc.} + M = 0,\] 
or \(X= 0\), in fact has \(m\) roots, they undertake to prove that \(X\) can be resolved into \(m\) simple factors. To this end, they assume \(m\) simple factors \(x-\alpha, x-\beta, x-\gamma\) etc. where \(\alpha, \beta, \gamma\) etc. are still unknown, and put the product of them equal to the function \(X\). Then from the comparison of the coefficients they deduce \(m\) equations, from which they say that the unknowns \(\alpha, \beta, \gamma\) etc. can be determined, since their population is also \(m\). m-1\) the unknowns can be eliminated, so that an equation emerges which, if it pleases, contains only the unknown.'  In order to say nothing of the rest which might be criticized in such an argument, I will only ask how we can be sure whether the last equation really has any root. Could it not be possible that neither this last equation nor the proposed quantity should be satisfied in the whole range of real and imaginary quantities? Moreover, experts will easily see that this last equation will necessarily be identical with the proposed one, if the calculation has been correctly set up; that is, after eliminating the unknowns \(\beta, \gamma\) etc. the equation 
\[ \alpha^m + A\alpha^{m-1} + B\alpha^{m-2} + \text{etc.} + M = 0 \]
should come out.  It is not necessary to explain more about this reasoning.

Some authors, who seem to have perceived the weakness of this method, assume it as an \textit{axiom}, that the equation really has, if not possible, then at least impossible roots.  But they do not seem to have explained clearly enough what they mean by possible and impossible quantities.  If possible quantities means real quantities, and impossible quantities means imaginary, then that axiom cannot be admitted at all, and it requires demonstration.   However, the expressions do not seem to be taken in that sense, but rather the meaning of the axiom seems to be this: `Although we are not yet certain, whether \(m\) real or imaginary quantities can always be found that satisfy a given equation of the \(m^{th}\) degree, we will suppose this for a while; for if by chance it should happen that so many real and imaginary quantities cannot be found, surely the departure will be obvious, so that we may say that the rest are impossible.'  If someone prefers to use this phrase rather than simply saying that in this case the equation would not have so many roots, then I cannot prevent it. But if he then uses these impossible roots as if they were something true, and e.g. says that the sum of all the roots of the equation \(x^m + A x^{m-1} + \text{etc.} = 0\) is \(= -A\), even if there are impossible roots among them (which expression properly means, \textit{even if they are deficient), then I cannot prove this by any means.  For impossible roots, accepted in this sense, are still roots, and then the axiom cannot in any way be admitted without demonstration, lest you foolishly doubt whether equations can exist which do not even have impossible roots?

\subsection*{4}

Before reviewing the demonstrations of our theorem by other geometers, and 


\end{document}
